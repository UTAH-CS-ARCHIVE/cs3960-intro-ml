# [Lecture Note] Optimization Theory

> Written by Hongseo Jang

## 1. What is an Optimization Problem?

In mathematics and computer science, an **optimization problem** is the problem of finding the best solution from all feasible solutions. In the context of machine learning, this almost always means finding the set of model parameters that minimizes an objective function.

Our goal is to find the optimal parameters, denoted $\boldsymbol{\theta}^*$, that minimize the cost function $J(\boldsymbol{\theta})$:
$$
\boldsymbol{\theta}^* = \arg\min_{\boldsymbol{\theta}} J(\boldsymbol{\theta})
$$
The entire training process of a machine learning model is essentially solving this optimization problem.

<br>

## 2. Loss Functions

A **Loss Function** (also called a Cost Function or Objective Function) is a function that maps an event or values of one or more variables onto a real number intuitively representing some "cost" associated with the event.

In supervised machine learning, the loss function quantifies the error between our model's predictions ($\hat{y}$) and the true ground-truth labels ($y$).
* A **high loss** value indicates that our model's predictions are far from the true values (the model is performing poorly).
* A **low loss** value indicates that our model's predictions are close to the true values (the model is performing well).

The choice of loss function depends on the task. For example:
* **Regression:** Mean Squared Error (MSE) is common. $J = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$.
* **Classification:** Cross-Entropy Loss is a standard choice.

The process of "learning" is the process of adjusting the model's parameters $\boldsymbol{\theta}$ to make the value of the loss function as small as possible.

<br>

## 3. Convex Functions in Optimization

A **convex function** is a function where the line segment connecting any two points on its graph lies on or above the graph. Visually, it has a "bowl" shape.



### Why Convexity is So Important

Convex functions have a critical property for optimization: **any local minimum is also a global minimum**.

* **Local Minimum:** A point that is lower than all of its immediate neighbors.
* **Global Minimum:** The lowest point in the entire domain of the function.

[Image comparing a convex function with one global minimum to a non-convex function with multiple local minima]

For a convex function, if we find a point where the gradient is zero (a flat spot), we can be confident that we have found the single best solution. If the function is *strictly* convex, this global minimum is also unique. This property guarantees that an optimization algorithm can converge to the optimal solution.

Many traditional machine learning models (like Linear Regression, Logistic Regression, SVMs) have convex loss functions. However, the loss landscapes for deep neural networks are highly **non-convex**, containing many local minima and saddle points, which makes optimization significantly more challenging.

<br>

## 4. Gradient Descent

Gradient Descent is an iterative first-order optimization algorithm used to find a local minimum of a differentiable function. It's the most common optimization algorithm in machine learning.

### Intuition

Imagine you are standing on a mountainside in a thick fog and you want to get to the bottom of the valley. You can't see the bottom, but you can feel the slope of the ground under your feet. The most straightforward strategy is:
1.  Check the direction of the steepest slope at your current position.
2.  Take a small step in the opposite direction (the steepest *downward* direction).
3.  Repeat this process.

This is exactly what Gradient Descent does.
* Your **position** is the current set of parameters $\boldsymbol{\theta}$.
* The **slope** is the gradient of the loss function, $\nabla J(\boldsymbol{\theta})$.
* The **direction of steepest descent** is the negative gradient, $-\nabla J(\boldsymbol{\theta})$.
* The **size of your step** is the learning rate, $\eta$.

### Mathematical Formulation

We want to update our current parameters $\boldsymbol{\theta}_t$ to a new set $\boldsymbol{\theta}_{t+1}$ by taking a small step. The goal is to make the loss at the new point smaller, i.e., $J(\boldsymbol{\theta}_{t+1}) < J(\boldsymbol{\theta}_t)$.

We can use a first-order Taylor expansion to approximate the loss function near our current point $\boldsymbol{\theta}_t$:
$$
J(\boldsymbol{\theta}) \approx J(\boldsymbol{\theta}_t) + (\boldsymbol{\theta} - \boldsymbol{\theta}_t)^T \nabla J(\boldsymbol{\theta}_t)
$$
To make $J(\boldsymbol{\theta})$ as small as possible, we need to make the term $(\boldsymbol{\theta} - \boldsymbol{\theta}_t)^T \nabla J(\boldsymbol{\theta}_t)$ as negative as possible. The dot product of two vectors is minimized when they point in exact opposite directions.

Therefore, the update step $(\boldsymbol{\theta}_{t+1} - \boldsymbol{\theta}_t)$ should be in the direction of the negative gradient, $-\nabla J(\boldsymbol{\theta}_t)$.

We introduce a small positive scalar $\eta$, the **learning rate**, to control the size of our step. This gives us the update rule:
$$
\boldsymbol{\theta}_{t+1} - \boldsymbol{\theta}_t = - \eta \nabla J(\boldsymbol{\theta}_t)
$$
Rearranging this gives the famous **Gradient Descent Update Rule**:
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J(\boldsymbol{\theta}_t)
$$

* $\boldsymbol{\theta}_t$: The vector of parameters at iteration $t$.
* $\eta$: The **learning rate**, a crucial hyperparameter. If it's too small, the model learns very slowly. If it's too large, the algorithm might overshoot the minimum and fail to converge.
* $\nabla J(\boldsymbol{\theta}_t)$: The gradient of the loss function with respect to the parameters, evaluated at $\boldsymbol{\theta}_t$. This tells us the direction of steepest ascent, which we use to find the direction to descend.