# [Lecture Note] Probability & Statistics II

> Written by Hongseo Jang

#### **1. Introduction**

Information theory, pioneered by Claude Shannon, is a branch of mathematics concerned with quantifying information. In machine learning, its concepts are not just theoretical but form the practical foundation for how we measure model performance and define objective functions. Key concepts like Entropy, Cross-Entropy, and KL Divergence allow us to measure uncertainty and the "distance" between probability distributions, which is fundamental to training models.

<br>

#### **2. Entropy: Measuring Uncertainty**

Entropy is a measure of the uncertainty, randomness, or "surprise" inherent in a probability distribution.
* A distribution with **high entropy** is very uncertain. For example, a fair coin flip ($P(\text{heads})=0.5, P(\text{tails})=0.5$) has high entropy because the outcome is maximally unpredictable.
* A distribution with **low entropy** is more certain. A biased coin that lands on heads 99% of the time has low entropy because the outcome is almost always known.

Mathematically, for a discrete random variable $X$ with a probability mass function $P(x)$, the entropy $H(P)$ is defined as the expected value of the "surprisal", $-\log P(x)$:

$$
H(P) = E_{x \sim P}[-\log P(x)] = -\sum_{i} P(x_i) \log P(x_i)
$$

The term $-\log P(x_i)$ can be thought of as the amount of surprise in observing outcome $x_i$. A low-probability event has a high surprisal, and a high-probability event has low surprisal. Entropy is thus the average surprisal of a distribution. The base of the logarithm determines the units; in machine learning, we typically use the natural logarithm (base $e$), with units called "nats".

<br>

#### **3. KL Divergence: Measuring the Difference Between Distributions**

The Kullback-Leibler (KL) Divergence measures how one probability distribution $Q$ differs from a reference (true) probability distribution $P$. It quantifies the "information loss" when we use distribution $Q$ to approximate the true distribution $P$.

For discrete probability distributions $P$ and $Q$, the KL divergence of $Q$ from $P$, denoted $D_{KL}(P\|Q)$, is defined as:

$$
D_{KL}(P\|Q) = \sum_{i} P(x_i) \log\left(\frac{P(x_i)}{Q(x_i)}\right)
$$

Key properties of KL Divergence:
* **Non-negative:** $D_{KL}(P\|Q) \ge 0$. It is zero if and only if $P$ and $Q$ are the same distribution.
* **Asymmetric:** In general, $D_{KL}(P\|Q) \neq D_{KL}(Q\|P)$. Therefore, it is not a true distance metric but rather a measure of divergence.

In machine learning, we can think of $P$ as the true distribution of our data and $Q$ as the distribution learned by our model. Our goal is to make our model's distribution $Q$ as close as possible to the true distribution $P$, which means we want to minimize $D_{KL}(P\|Q)$.

<br>

#### **4. Cross-Entropy: A Practical Proxy for KL Divergence**

We can expand the KL Divergence formula to reveal its relationship with Entropy and a new quantity, Cross-Entropy.

$$
D_{KL}(P\|Q) = \sum_{i} P(x_i) (\log P(x_i) - \log Q(x_i))
$$

$$
= \sum_{i} P(x_i) \log P(x_i) - \sum_{i} P(x_i) \log Q(x_i)
$$

$$
= -\left(-\sum_{i} P(x_i) \log P(x_i)\right) - \sum_{i} P(x_i) \log Q(x_i)
$$

This gives us the important relationship:

$$
D_{KL}(P\|Q) = -H(P) + H(P, Q)
$$

Where:
* $H(P)$ is the entropy of the true distribution $P$.
* $H(P, Q) = -\sum_{i} P(x_i) \log Q(x_i)$ is the **Cross-Entropy** between $P$ and $Q$.

The cross-entropy $H(P, Q)$ represents the average number of nats required to encode an event drawn from the true distribution $P$, using a code optimized for the approximate distribution $Q$.

When we train a model, the true data distribution $P$ is fixed. This means its entropy, $H(P)$, is a constant. Therefore, minimizing the KL Divergence between our model's predictions $Q$ and the true distribution $P$ is equivalent to minimizing the Cross-Entropy $H(P, Q)$.

**Minimize $D_{KL}(P\|Q)$ $\iff$ Minimize $H(P, Q)$** (since $H(P)$ is constant)

This makes Cross-Entropy a very useful and efficient loss function.

<br>

#### **5. Deriving Cross-Entropy Loss from Maximum Likelihood Estimation (MLE)**

The use of cross-entropy as a loss function for classification is not arbitrary; it is directly derived from the principle of Maximum Likelihood Estimation (MLE).

Let's consider a classification problem with a dataset of $N$ i.i.d. samples $\{(\mathbf{x}_i, y_i)\}_{i=1}^N$. Our model, with parameters $\theta$, outputs a probability distribution over the classes, $Q(y | \mathbf{x}; \theta)$.

The goal of **MLE** is to find the parameters $\theta$ that maximize the likelihood of observing the given dataset. The likelihood function is:

$$
\mathcal{L}(\theta) = P(\text{Data}|\theta) = \prod_{i=1}^{N} Q(y_i | \mathbf{x}_i; \theta)
$$

For numerical stability and mathematical convenience, we work with the log-likelihood:

$$
\log \mathcal{L}(\theta) = \sum_{i=1}^{N} \log Q(y_i | \mathbf{x}_i; \theta)
$$

Maximizing the log-likelihood is equivalent to minimizing the **negative log-likelihood (NLL)**:

$$
\text{Loss} = \text{NLL} = -\sum_{i=1}^{N} \log Q(y_i | \mathbf{x}_i; \theta)
$$

Now, let's connect this to cross-entropy. In classification, the true label $y_i$ can be represented by a one-hot encoded vector $P_i$. For example, if there are 3 classes and the true label is class 2, then $P_i = [0, 1, 0]$. So, $P_i(c)=1$ if $c$ is the true class, and $P_i(c)=0$ otherwise.

Let's look at the loss term for a single data point $(\mathbf{x}_i, y_i)$: $-\log Q(y_i | \mathbf{x}_i; \theta)$. We can rewrite this as a sum over all possible classes $c$:

$$
\text{Loss}_i = -\sum_{c \in \text{Classes}} P_i(c) \log Q(y=c | \mathbf{x}_i; \theta)
$$

This works because $P_i(c)$ is zero for all incorrect classes, so those terms vanish, and one for the single correct class, which leaves us with the original term. This final expression is exactly the definition of the cross-entropy $H(P_i, Q_i)$ between the true one-hot distribution $P_i$ and the model's predicted distribution $Q_i$.

Therefore, **minimizing the negative log-likelihood is identical to minimizing the cross-entropy loss**. This elegant connection provides a strong theoretical justification for why cross-entropy is the standard, natural choice for a loss function in classification tasks.