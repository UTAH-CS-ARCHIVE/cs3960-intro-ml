# [Lecture Note] Linear Algebra III - Lecture Notes

> Written by Hongseo Jang

## 1. Eigenvalues and Eigenvectors

Eigenvalues and eigenvectors are special properties of a square matrix. They are fundamental to understanding matrix transformations, and they appear in many applications, including physics, engineering, and machine learning.

### Definition

For a given square matrix $\mathbf{A}$ of size $n \times n$, a non-zero vector $\mathbf{v}$ is an **eigenvector** of $\mathbf{A}$ if it satisfies the following equation for some scalar $\lambda$:

$$
\mathbf{A}\mathbf{v} = \lambda\mathbf{v}
$$

* $\mathbf{v}$: The Eigenvector (a non-zero vector).
* $\lambda$: The Eigenvalue (a scalar).

### Geometric Meaning

The geometric interpretation is key to understanding this concept. A matrix $\mathbf{A}$ can be seen as a linear transformation that maps vectors to new vectors. An **eigenvector** of $\mathbf{A}$ is a special vector whose direction is **unchanged** by the transformation. When the matrix $\mathbf{A}$ acts on its eigenvector $\mathbf{v}$, the output is simply the original vector $\mathbf{v}$ scaled by a factor of $\lambda$.

* If $\lambda > 1$, the eigenvector is stretched.
* If $0 < \lambda < 1$, the eigenvector is shrunk.
* If $\lambda < 0$, the eigenvector's direction is reversed.

Essentially, eigenvectors represent the "axes of transformation" for the matrix $\mathbf{A}$. They are the directions that are preserved under the transformation.

To find these, we solve the characteristic equation $\det(\mathbf{A} - \lambda\mathbf{I}) = 0$ to find the eigenvalues $\lambda$, and then we solve the system $(\mathbf{A} - \lambda\mathbf{I})\mathbf{v} = 0$ for each $\lambda$ to find the corresponding eigenvectors $\mathbf{v}$.

<br>

## 2. Eigendecomposition

Eigendecomposition is the process of factoring a matrix into a product of its eigenvalues and eigenvectors. This is possible only for diagonalizable matrices (a class of square matrices).

If an $n \times n$ matrix $\mathbf{A}$ has $n$ linearly independent eigenvectors, it can be decomposed as:

$$
\mathbf{A} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}
$$

Where:
* $\mathbf{Q}$ is a square matrix whose columns are the eigenvectors of $\mathbf{A}$.
* $\mathbf{\Lambda}$ (Lambda) is a diagonal matrix whose diagonal elements are the corresponding eigenvalues of $\mathbf{A}$. The order of eigenvalues in $\mathbf{\Lambda}$ must match the order of eigenvectors in $\mathbf{Q}$.
* $\mathbf{Q}^{-1}$ is the inverse of matrix $\mathbf{Q}$.

This decomposition reveals the fundamental properties of the transformation represented by $\mathbf{A}$. It effectively changes the basis to the basis of eigenvectors, applies a simple scaling (the eigenvalues), and then changes the basis back.

<br>

## 3. Singular Value Decomposition (SVD)

Eigendecomposition is powerful but limited to certain square matrices. **Singular Value Decomposition (SVD)** is a more general factorization that works for *any* rectangular matrix ($m \times n$). It is one of the most important and useful matrix decompositions in linear algebra.

The SVD of a matrix $\mathbf{A}$ is given by:

$$
\mathbf{A} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T
$$

Where:
* $\mathbf{A}$ is the $m \times n$ matrix to be decomposed.
* $\mathbf{U}$ is an $m \times m$ orthogonal matrix. Its columns are the *left-singular vectors*, which are the eigenvectors of the matrix $\mathbf{A}\mathbf{A}^T$.
* $\boldsymbol{\Sigma}$ (Sigma) is an $m \times n$ rectangular diagonal matrix. The diagonal entries, called **singular values**, are the square roots of the non-zero eigenvalues of both $\mathbf{A}\mathbf{A}^T$ and $\mathbf{A}^T\mathbf{A}$. They are arranged in descending order by convention.
* $\mathbf{V}$ is an $n \times n$ orthogonal matrix. Its columns are the *right-singular vectors*, which are the eigenvectors of the matrix $\mathbf{A}^T\mathbf{A}$. $\mathbf{V}^T$ is its transpose.

SVD states that any linear transformation can be broken down into three fundamental operations: a rotation ($\mathbf{V}^T$), followed by a scaling along orthogonal axes ($\boldsymbol{\Sigma}$), followed by another rotation ($\mathbf{U}$).

<br>

## 4. SVD's Connection to Principal Component Analysis (PCA)

PCA is a dimensionality reduction technique used to reduce the number of features in a dataset while retaining as much information (variance) as possible. SVD is the mathematical engine that powers PCA.

* **Goal of PCA:** To find a new set of orthogonal axes, called principal components, that align with the directions of maximum variance in the data.
* **The Connection:** The principal components of a data matrix are the eigenvectors of its covariance matrix. SVD gives us a direct way to find these components without having to compute the covariance matrix explicitly.

Let's say we have a data matrix $\mathbf{X}$ (centered to have a mean of zero for each feature). The SVD of this matrix is $\mathbf{X} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T$.

1.  **Principal Components:** The columns of the matrix $\mathbf{V}$ (the right-singular vectors) are the **principal components** of the dataset. The first column of $\mathbf{V}$ is the first principal component (PC1), the direction of highest variance. The second column is PC2, and so on.
2.  **Variance Explained:** The singular values in $\boldsymbol{\Sigma}$ are directly related to the amount of variance captured by each principal component. The squared singular values are proportional to the eigenvalues of the covariance matrix.
3.  **Dimensionality Reduction:** To reduce the data to $k$ dimensions, we perform a *truncated SVD*. We take the first $k$ columns of $\mathbf{U}$, the first $k$ columns and rows of $\boldsymbol{\Sigma}$, and the first $k$ columns of $\mathbf{V}$.
    $$
    \mathbf{X}_k = \mathbf{U}_k \boldsymbol{\Sigma}_k \mathbf{V}_k^T
    $$
    This new matrix $\mathbf{X}_k$ is the best rank-$k$ approximation of the original data. To get the new, lower-dimensional representation of our data, we project the original data onto the first $k$ principal components: $\mathbf{X}_{reduced} = \mathbf{X} \mathbf{V}_k$.