## [Lecture Note] Lecture Notes: Matrix Decomposition Techniques


### LU Decomposition

**1. Concept**

LU decomposition factors a square matrix $\mathbf{A}$ into the product of a **lower triangular matrix** $\mathbf{L}$ and an **upper triangular matrix** $\mathbf{U}$.

$$
\mathbf{A} = \mathbf{LU}
$$

-   **L (Lower Triangular Matrix):** A square matrix where all entries above the main diagonal are zero. In the most common form of LU decomposition (Doolittle decomposition), the diagonal elements of $\mathbf{L}$ are all 1s. This makes $\mathbf{L}$ a unit lower triangular matrix.
-   **U (Upper Triangular Matrix):** A square matrix where all entries below the main diagonal are zero.

For a 3x3 matrix, the decomposition would look like this:

$$
\begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{pmatrix} = \begin{pmatrix} 1 & 0 & 0 \\ l_{21} & 1 & 0 \\ l_{31} & l_{32} & 1 \end{pmatrix} \begin{pmatrix} u_{11} & u_{12} & u_{13} \\ 0 & u_{22} & u_{23} \\ 0 & 0 & u_{33} \end{pmatrix}
$$

**2. Mathematical Principle and Connection to Gaussian Elimination**

LU decomposition is essentially a matrix representation of Gaussian elimination. The process of using row operations to transform matrix $\mathbf{A}$ into an upper triangular form (echelon form) results in the matrix $\mathbf{U}$. The matrix $\mathbf{L}$ is constructed from the multipliers used in these row operations. Each elementary row operation (like subtracting a multiple of one row from another) can be represented by an elementary matrix. The product of the inverses of these elementary matrices forms the lower triangular matrix $\mathbf{L}$.

**3. Application: Solving Linear Systems**

The primary advantage of LU decomposition is in efficiently solving systems of linear equations, represented as $\mathbf{Ax} = \mathbf{b}$.

If we have the decomposition $\mathbf{A} = \mathbf{LU}$, we can rewrite the equation as:

$$
\mathbf{LUx} = \mathbf{b}
$$

We can solve this in a two-step process:
First, let $\mathbf{y} = \mathbf{Ux}$. Then the equation becomes:

1.  **Solve for $\mathbf{y}$ in $\mathbf{Ly} = \mathbf{b}$:** This process is called **forward substitution**. Since $\mathbf{L}$ is lower triangular, we can solve for $y_1$ first, then substitute it to find $y_2$, and so on. This is computationally very cheap.

    $$
    \begin{pmatrix} 1 & 0 & 0 \\ l_{21} & 1 & 0 \\ l_{31} & l_{32} & 1 \end{pmatrix} \begin{pmatrix} y_1 \\ y_2 \\ y_3 \end{pmatrix} = \begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix}
    $$

2.  **Solve for $\mathbf{x}$ in $\mathbf{Ux} = \mathbf{y}$:** Once we have $\mathbf{y}$, we solve for $\mathbf{x}$. This is called **backward substitution**. Since $\mathbf{U}$ is upper triangular, we can solve for the last variable $x_n$ first, then substitute it back to find $x_{n-1}$, and so on. This is also computationally efficient.

    $$
    \begin{pmatrix} u_{11} & u_{12} & u_{13} \\ 0 & u_{22} & u_{23} \\ 0 & 0 & u_{33} \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} y_1 \\ y_2 \\ y_3 \end{pmatrix}
    $$

This two-step process is much faster than directly computing the inverse of $\mathbf{A}$ (i.e., $\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}$), especially when we need to solve the system for multiple different vectors $\mathbf{b}$ with the same matrix $\mathbf{A}$. The decomposition is done only once.

**4. Existence and Pivoting**

A square matrix $\mathbf{A}$ has an LU decomposition if all its leading principal minors are non-zero. However, this condition is not always met. Furthermore, for numerical stability, it's often necessary to perform row swaps during elimination. This leads to a slightly modified form called **PLU decomposition**, where $\mathbf{P}$ is a permutation matrix that accounts for the row interchanges.

$$
\mathbf{A} = \mathbf{PLU}
$$

This factorization always exists for any invertible matrix.

<br>

### QR Decomposition

**1. Concept**

QR decomposition (or QR factorization) factors a matrix $\mathbf{A}$ (not necessarily square) into the product of an **orthogonal matrix** $\mathbf{Q}$ and an **upper triangular matrix** $\mathbf{R}$.

$$
\mathbf{A} = \mathbf{QR}
$$

-   **Q (Orthogonal Matrix):** A square matrix whose columns are orthonormal vectors. This means the columns are mutually perpendicular and each has a length of 1. A key property of an orthogonal matrix is that its inverse is equal to its transpose:
    $$
    \mathbf{Q}^T\mathbf{Q} = \mathbf{I} \quad \Rightarrow \quad \mathbf{Q}^{-1} = \mathbf{Q}^T
    $$
-   **R (Upper Triangular Matrix):** A matrix where all entries below the main diagonal are zero.

**2. Mathematical Principle and Connection to Gram-Schmidt**

The QR decomposition can be understood as a process of "orthogonalizing" the column vectors of matrix $\mathbf{A}$. The most common method to compute this is the **Gram-Schmidt process**.

Let the column vectors of $\mathbf{A}$ be $[\mathbf{a}_1, \mathbf{a}_2, ..., \mathbf{a}_n]$. The Gram-Schmidt process takes these vectors and produces an orthonormal set of vectors $[\mathbf{q}_1, \mathbf{q}_2, ..., \mathbf{q}_n]$ that form the columns of $\mathbf{Q}$. Each original vector $\mathbf{a}_i$ can then be expressed as a linear combination of the orthonormal vectors $\mathbf{q}_1, ..., \mathbf{q}_i$. These coefficients form the entries of the upper triangular matrix $\mathbf{R}$.

Other methods like Householder reflections or Givens rotations are often used in practice because they are more numerically stable than the Gram-Schmidt process.

**3. Application: Solving Least Squares Problems**

QR decomposition is extremely useful for solving linear least squares problems, which are fundamental to machine learning models like linear regression. The goal is to find an $\mathbf{x}$ that minimizes the squared Euclidean norm of the residual, $\|\mathbf{Ax} - \mathbf{b}\|^2$.

The solution to the least squares problem is given by the normal equation:

$$
\mathbf{A}^T\mathbf{Ax} = \mathbf{A}^T\mathbf{b}
$$

Solving this directly by first computing $\mathbf{A}^T\mathbf{A}$ can be numerically unstable, as the condition number of $\mathbf{A}^T\mathbf{A}$ can be very large. QR decomposition provides a more stable alternative.

By substituting $\mathbf{A} = \mathbf{QR}$ into the normal equation:

$$
(\mathbf{QR})^T(\mathbf{QR})\mathbf{x} = (\mathbf{QR})^T\mathbf{b}
$$

$$
\mathbf{R}^T\mathbf{Q}^T\mathbf{Q}\mathbf{Rx} = \mathbf{R}^T\mathbf{Q}^T\mathbf{b}
$$

Since $\mathbf{Q}^T\mathbf{Q} = \mathbf{I}$, this simplifies to:

$$
\mathbf{R}^T\mathbf{Rx} = \mathbf{R}^T\mathbf{Q}^T\mathbf{b}
$$

If $\mathbf{A}$ has full column rank, then $\mathbf{R}$ is invertible. We can multiply by $(\mathbf{R}^T)^{-1}$ on the left:

$$
\mathbf{Rx} = \mathbf{Q}^T\mathbf{b}
$$

This final system is an upper triangular system, which can be solved efficiently for $\mathbf{x}$ using backward substitution, just like in the LU decomposition case. This method avoids the explicit formation of $\mathbf{A}^T\mathbf{A}$ and is therefore preferred for its numerical stability.