# [Lecture Note] Calculus for Machine Learning

> Written by Hongseo Jang

#### **1. Beyond the Gradient: Introducing Curvature**

In our study of optimization, we have heavily relied on the first-order derivative of a multivariate function $f: \mathbb{R}^n \to \mathbb{R}$, which is the **gradient**, denoted by $\nabla f$. The gradient vector tells us two crucial things at any point $\mathbf{x}$:
1.  **Direction:** It points in the direction of the steepest ascent of the function.
2.  **Magnitude:** Its magnitude, $\|\nabla f\|$, indicates the rate of this ascent.

First-order optimization methods like Gradient Descent use this information to iteratively move in the opposite direction of the gradient to find a local minimum. However, the gradient only provides a linear, local approximation of the function. It tells us about the "slope" of the surface, but it gives us no information about its "curvature". Is the slope increasing or decreasing? Are we in a flat plateau, a narrow valley, or on a steep hill?

To answer these questions, we need to look at the second-order derivatives. Just as the second derivative $f''(x)$ in single-variable calculus tells us about the concavity of a curve, its multivariate equivalent gives us a richer picture of the function's landscape, which is essential for understanding the complexities of optimization.

<br>

#### **2. The Hessian Matrix**

The second-order derivative of a scalar-valued function $f(\mathbf{x})$ with respect to a vector $\mathbf{x}$ is the **Hessian matrix**, denoted by $\mathbf{H}$ or $\nabla^2 f(\mathbf{x})$. It is a square matrix containing all possible second-order partial derivatives of the function.

For a function $f: \mathbb{R}^n \to \mathbb{R}$, the Hessian is an $n \times n$ matrix defined as:

$$
\mathbf{H} =
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

An important property of the Hessian arises from Schwarz's theorem (or Clairaut's theorem on equality of mixed partials), which states that if the second partial derivatives are continuous, then the order of differentiation does not matter:

$$
\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}
$$

This means that the Hessian matrix is always a **symmetric matrix** ($\mathbf{H} = \mathbf{H}^T$). This property is fundamental, as it guarantees that its eigenvalues are real and that it is orthogonally diagonalizable.

<br>

#### **3. Classifying Critical Points with the Hessian**

A **critical point** (or stationary point) $\mathbf{x}_0$ of a function is a point where the gradient is the zero vector, i.e., $\nabla f(\mathbf{x}_0) = \mathbf{0}$. At such a point, the function's surface is momentarily "flat". This point could be a local minimum, a local maximum, or a saddle point. The gradient alone cannot distinguish between these cases.

This is where the Hessian comes in. The **Second Derivative Test** for multivariate functions involves analyzing the properties of the Hessian matrix evaluated at the critical point, $\mathbf{H}(\mathbf{x}_0)$. Specifically, we examine its eigenvalues, which tell us about the curvature in the directions of the corresponding eigenvectors.

Let $\lambda_1, \lambda_2, \dots, \lambda_n$ be the eigenvalues of $\mathbf{H}(\mathbf{x}_0)$:

1.  **Local Minimum:** If all eigenvalues are strictly **positive** ($\lambda_i > 0$ for all $i$), the Hessian is **positive definite**. This means the function curves upwards in every direction from $\mathbf{x}_0$. Thus, $\mathbf{x}_0$ is a **local minimum**.

2.  **Local Maximum:** If all eigenvalues are strictly **negative** ($\lambda_i < 0$ for all $i$), the Hessian is **negative definite**. The function curves downwards in every direction from $\mathbf{x}_0$. Thus, $\mathbf{x}_0$ is a **local maximum**.

3.  **Saddle Point:** If some eigenvalues are positive and some are negative, the Hessian is **indefinite**. The function curves upwards in some directions and downwards in others, resembling a horse's saddle. Thus, $\mathbf{x}_0$ is a **saddle point**.

4.  **Inconclusive:** If any eigenvalue is zero (and the non-zero eigenvalues share the same sign), the Hessian is **semi-definite** (positive or negative). The test is inconclusive, and further analysis is required to classify the point.

<br>

#### **4. Relevance to Deep Learning Optimization**

The concepts of the gradient and Hessian are central to understanding how we train neural networks. The loss function, $L(\mathbf{w})$, is a highly complex, high-dimensional function of the model's weights, $\mathbf{w}$.

* **First-Order Methods (e.g., SGD):** These methods update weights using only the gradient: $\mathbf{w}_{k+1} = \mathbf{w}_k - \alpha \nabla L(\mathbf{w}_k)$. They are computationally cheap but are "blind" to the curvature of the loss landscape. This can lead to slow convergence in flat regions or oscillation across narrow valleys.

* **Second-Order Methods (e.g., Newton's Method):** These methods incorporate curvature information via the Hessian. The update rule for Newton's method is:

    $$
    \mathbf{w}_{k+1} = \mathbf{w}_k - \alpha [\mathbf{H}(\mathbf{w}_k)]^{-1} \nabla L(\mathbf{w}_k)
    $$

    This method essentially fits a quadratic approximation to the loss surface at the current point and jumps to the minimum of that approximation. This often results in much faster convergence (fewer iterations).

However, there is a major problem: **scalability**. For a neural network with $n$ parameters (where $n$ can be in the millions or billions), the Hessian matrix has $n^2$ elements. Computing, storing, and inverting such a massive matrix at every iteration is computationally prohibitive.

Because of this, pure second-order methods are rarely used in deep learning. Instead, the field relies on:
* **First-order methods** with adaptive learning rates (e.g., Adam, RMSprop) that try to indirectly account for curvature.
* **Quasi-Newton methods** (e.g., L-BFGS) that build an approximation of the inverse Hessian from a history of gradient evaluations, avoiding the need to ever explicitly form the full matrix.

Understanding the Hessian is crucial not for direct application, but for characterizing the challenges of optimization. The loss landscapes of deep networks are non-convex and are now understood to be rife with saddle points, not local minima. The Hessian gives us the mathematical language to describe these saddle points and understand why they can be problematic for optimizers.