# [Lecture Note] Linear Algebra III

> Written by Hongseo Jang

#### **1. Introduction to PCA**

Principal Component Analysis (PCA) is a fundamental dimensionality reduction technique used in machine learning and data analysis. Its primary goal is to transform a high-dimensional dataset into a lower-dimensional space while preserving as much of the original data's variance as possible. This is achieved by identifying a new set of orthogonal axes, called principal components, that are ordered by the amount of variance they explain. The first principal component (PC1) captures the largest possible variance, the second principal component (PC2) captures the second-largest variance under the constraint that it is orthogonal to PC1, and so on.

<br>

#### **2. The Objective: Maximizing Variance**

Let's consider a dataset represented by a matrix $\mathbf{X}$ of size $n \times p$, where $n$ is the number of data points and $p$ is the number of features. We assume that the data has been centered by subtracting the mean of each feature, so each column of $\mathbf{X}$ has a mean of zero.

Our goal is to find a direction, represented by a unit vector $\mathbf{w}$ (where $\|\mathbf{w}\| = 1$), onto which the projection of the data has the maximum possible variance.

A single data point $\mathbf{x}_i$ (a row vector in $\mathbf{X}$) is projected onto the direction $\mathbf{w}$ by the dot product $\mathbf{x}_i \mathbf{w}$. The set of all projections forms a new vector $\mathbf{Xw}$. Since the original data $\mathbf{X}$ is mean-centered, the projected data $\mathbf{Xw}$ will also have a mean of zero.

The variance of these projected points is given by:

$$
\text{Var}(\mathbf{Xw}) = \frac{1}{n-1} \sum_{i=1}^{n} (\mathbf{x}_i \mathbf{w} - \bar{\mathbf{x}}\mathbf{w})^2
$$

Since the mean is zero, this simplifies to:

$$
\text{Var}(\mathbf{Xw}) = \frac{1}{n-1} \sum_{i=1}^{n} (\mathbf{x}_i \mathbf{w})^2 = \frac{1}{n-1} (\mathbf{Xw})^T(\mathbf{Xw})
$$

$$
= \frac{1}{n-1} \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}
$$

We recognize the term $\frac{1}{n-1} \mathbf{X}^T \mathbf{X}$ as the sample covariance matrix of the data, which we denote as $\mathbf{C}$.

$$
\mathbf{C} = \frac{1}{n-1} \mathbf{X}^T \mathbf{X}
$$

So, the variance of the projected data is $\mathbf{w}^T \mathbf{C} \mathbf{w}$.

Our optimization problem is therefore:

**Maximize** $\mathbf{w}^T \mathbf{C} \mathbf{w}$
**Subject to** $\mathbf{w}^T \mathbf{w} = 1$

<br>

#### **3. Solving the Optimization Problem: Eigenvalue Decomposition**

To solve this constrained optimization problem, we can use the method of Lagrange multipliers. We define the Lagrangian function $\mathcal{L}$:

$$
\mathcal{L}(\mathbf{w}, \lambda) = \mathbf{w}^T \mathbf{C} \mathbf{w} - \lambda (\mathbf{w}^T \mathbf{w} - 1)
$$

To find the maximum, we take the derivative of $\mathcal{L}$ with respect to $\mathbf{w}$ and set it to zero.

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = 2\mathbf{C}\mathbf{w} - 2\lambda\mathbf{w} = 0
$$

This simplifies to the fundamental eigenvalue equation:

$$
\mathbf{C}\mathbf{w} = \lambda\mathbf{w}
$$

This result is significant. It tells us that the direction vector $\mathbf{w}$ that maximizes the variance must be an **eigenvector** of the covariance matrix $\mathbf{C}$. The scalar $\lambda$ is the corresponding **eigenvalue**.

Which eigenvector do we choose? Let's substitute this result back into our objective function (the variance):

$$
\text{Variance} = \mathbf{w}^T \mathbf{C} \mathbf{w} = \mathbf{w}^T (\lambda\mathbf{w}) = \lambda (\mathbf{w}^T \mathbf{w})
$$

Since we have the constraint that $\mathbf{w}^T \mathbf{w} = 1$, the variance simplifies to:

$$
\text{Variance} = \lambda
$$

This reveals a crucial insight: the variance of the data projected onto an eigenvector is equal to the corresponding eigenvalue. Therefore, to maximize the variance, we must choose the eigenvector $\mathbf{w}$ that corresponds to the **largest eigenvalue** $\lambda$. This eigenvector is the first principal component (PC1).

The second principal component is the eigenvector corresponding to the second-largest eigenvalue, and so on. These eigenvectors (principal components) are orthogonal to each other because the covariance matrix $\mathbf{C}$ is symmetric, a property of symmetric matrices.

<br>

#### **4. Geometric Interpretation**

Geometrically, the eigenvectors of the covariance matrix represent the principal axes of the ellipsoid that best fits the data cloud. The length of each axis is proportional to the square root of the corresponding eigenvalue. The first principal component is the direction of the longest axis of this ellipsoid, representing the direction of greatest variance in the data.

<br>

#### **5. The Connection to Singular Value Decomposition (SVD)**

While eigenvalue decomposition of the covariance matrix is the classical way to derive PCA, it is often more numerically stable and efficient to use Singular Value Decomposition (SVD) directly on the data matrix $\mathbf{X}$.

The SVD of our mean-centered $n \times p$ data matrix $\mathbf{X}$ is given by:

$$
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T
$$

Where:
* $\mathbf{U}$ is an $n \times n$ orthogonal matrix whose columns are the left-singular vectors.
* $\mathbf{\Sigma}$ is an $n \times p$ rectangular diagonal matrix containing the singular values $\sigma_i$ in descending order.
* $\mathbf{V}$ is a $p \times p$ orthogonal matrix whose columns are the right-singular vectors.

Now, let's look at the covariance matrix $\mathbf{C}$ again in terms of the SVD of $\mathbf{X}$:

$$
\mathbf{C} = \frac{1}{n-1} \mathbf{X}^T \mathbf{X} = \frac{1}{n-1} (\mathbf{U} \mathbf{\Sigma} \mathbf{V}^T)^T (\mathbf{U} \mathbf{\Sigma} \mathbf{V}^T)
$$

$$
= \frac{1}{n-1} (\mathbf{V} \mathbf{\Sigma}^T \mathbf{U}^T) (\mathbf{U} \mathbf{\Sigma} \mathbf{V}^T)
$$

Since $\mathbf{U}$ is orthogonal, $\mathbf{U}^T \mathbf{U} = \mathbf{I}$ (the identity matrix). The expression simplifies to:

$$
\mathbf{C} = \frac{1}{n-1} \mathbf{V} (\mathbf{\Sigma}^T \mathbf{\Sigma}) \mathbf{V}^T
$$

The term $\mathbf{\Sigma}^T \mathbf{\Sigma}$ is a $p \times p$ square diagonal matrix with the squared singular values $\sigma_i^2$ on its diagonal. Let's call this diagonal matrix $\mathbf{S}^2$.

$$
\mathbf{C} = \mathbf{V} \left( \frac{\mathbf{S}^2}{n-1} \right) \mathbf{V}^T
$$

This is exactly the form of the eigenvalue decomposition of $\mathbf{C}$, where:
1.  The columns of $\mathbf{V}$ are the eigenvectors of $\mathbf{C}$. These are the **principal components**.
2.  The diagonal entries of $\frac{\mathbf{S}^2}{n-1}$ are the eigenvalues of $\mathbf{C}$. Thus, the relationship between an eigenvalue $\lambda_i$ and its corresponding singular value $\sigma_i$ is $\lambda_i = \frac{\sigma_i^2}{n-1}$.

This deep connection means we can find the principal components (the columns of $\mathbf{V}$) by performing SVD on the data matrix $\mathbf{X}$ without ever needing to compute the covariance matrix $\mathbf{C}$. This direct approach is generally preferred as it avoids the potential loss of numerical precision that can occur when computing $\mathbf{X}^T \mathbf{X}$.

Furthermore, the principal component scores, which are the coordinates of the data in the new basis, can be computed as $\mathbf{T} = \mathbf{X}\mathbf{V}$. Using SVD, this becomes:

$$
\mathbf{T} = (\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T)\mathbf{V} = \mathbf{U}\mathbf{\Sigma}(\mathbf{V}^T\mathbf{V}) = \mathbf{U}\mathbf{\Sigma}
$$

So, the SVD gives us both the principal components (in $\mathbf{V}$) and the projected data scores (in $\mathbf{U}\mathbf{\Sigma}$) directly.