# [Lecture Notes] Advanced Lecture Note 2: Data and Representation

## A. Feature Space and the Curse of Dimensionality

### A.1 Feature Map and Representation
- Representation: data point  

$$
x \in \mathcal{X}, \quad \phi: \mathcal{X} \to \mathbb{R}^d, \quad \phi(x) \in \mathbb{R}^d
$$  

- Hypothesis space:  

$$
\mathcal{H} = \{ f(\cdot) = \langle w, \phi(\cdot) \rangle : w \in \mathbb{R}^d \}
$$  

- Manifold hypothesis: data lies near a manifold of dimension  

$$
m \ll d
$$

### A.2 High-Dimensional Geometry: Volume and Concentration
- Cube boundary volume ratio:  

$$
\text{Ratio} = 1 - (1 - 2\varepsilon)^d \xrightarrow[d \to \infty]{} 1
$$  

- Volume of unit ball:  

$$
V_d(r) = \frac{\pi^{d/2}}{\Gamma(\tfrac{d}{2}+1)} r^d
$$  

- Concentration of norm for Gaussian:  

$$
X \sim \mathcal{N}(0, I_d) \quad \Rightarrow \quad \|X\|_2^2 \sim \chi^2(d), \quad \|X\|_2 \approx \sqrt{d}
$$  

- Expected distance in cube:  

$$
\mathbb{E}\|X-c\|_2^2 = \frac{d}{12}
$$  

- Nearest-neighbor scale:  

$$
r \approx \left(\frac{\log n}{n}\right)^{1/d}
$$

### A.3 Practical Impact
- k-NN, RBF kernels, density estimation degrade as \(d\) grows.  
- Strategies: dimension reduction, representation learning, regularization, metric learning.

<br>

## B. Preprocessing, Normalization, and Scaling: Mathematical Motivation

### B.1 Standardization and Condition Number
- Standardization:  

$$
\tilde{x}_j = \frac{x_j - \mu_j}{\sigma_j}
$$  

- Ridge regression normal equation:  

$$
\hat{w} = (X^\top X + \lambda I)^{-1} X^\top y
$$  

- Without scaling, condition number  

$$
\kappa(X^\top X)
$$  

is large, leading to instability.

### B.2 Centering and PCA
- Centering:  

$$
x \leftarrow x - \mu
$$  

- PCA based on covariance:  

$$
\Sigma = \frac{1}{n} X^\top X
$$  

### B.3 Whitening and Mahalanobis Distance
- Whitening transform:  

$$
\tilde{x} = \Sigma^{-1/2}(x - \mu), \quad \mathrm{Cov}(\tilde{x}) = I
$$  

- Mahalanobis distance:  

$$
d_M(x,y)^2 = (x-y)^\top \Sigma^{-1}(x-y) = \|\Sigma^{-1/2}(x-y)\|_2^2
$$  

### B.4 Normalization
- L2 normalization:  

$$
\hat{x} = \frac{x}{\|x\|_2}
$$  

- Minâ€“Max scaling:  

$$
\tilde{x} = \frac{x - \min}{\max - \min}
$$  

- Robust scaling:  

$$
\tilde{x} = \frac{x - \text{median}}{\text{MAD}}
$$  

### B.5 Preventing Data Leakage
- Fit scalers only on training data, then transform all splits.

<br>

## C. Inner Product Space and Similarity

### C.1 Properties
- Norm from inner product:  

$$
\|x\| = \sqrt{\langle x, x \rangle}
$$  

- Distance:  

$$
d(x,y) = \|x-y\|
$$  

- Cosine similarity:  

$$
\cos\theta = \frac{\langle x, y \rangle}{\|x\|\,\|y\|}
$$  

### C.2 Similarity vs Distance
- Euclidean distance via inner product:  

$$
\|x-y\|^2 = \|x\|^2 + \|y\|^2 - 2\langle x, y \rangle
$$  

- Bregman divergence:  

$$
D_\phi(x,y) = \phi(x) - \phi(y) - \langle \nabla \phi(y), x-y \rangle
$$  

### C.3 Kernel Perspective
- PSD kernel condition:  

$$
\sum_{i,j} \alpha_i \alpha_j k(x_i, x_j) \ge 0
$$  

- Examples:  

$$
k(x,y) = x^\top y, \quad (x^\top y + c)^p, \quad \exp\left(-\frac{\|x-y\|^2}{2\sigma^2}\right)
$$  

### C.4 Metric Learning
- Generalized Mahalanobis distance:  

$$
d_M(x,y)^2 = (x-y)^\top M (x-y), \quad M \succeq 0
$$  

- Factorization:  

$$
M = L^\top L \quad \Rightarrow \quad d_M(x,y) = \|Lx - Ly\|_2
$$