# [Lecture Note] Introduction & Linear Algebra I

> Written by Hongseo Jang


## 1. Projection and Orthogonality

**Projection** is a fundamental concept that allows us to find the component of one vector that lies in the direction of another. Imagine shining a light from directly above onto a vector **u**. The shadow it casts on the line defined by vector **v** is the projection of **u** onto **v**.

The formula to calculate the projection of vector **u** onto vector **v** is given by:

$$
\text{proj}_{\mathbf{v}}\mathbf{u} = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{v}\|^2}\mathbf{v}
$$

Let's break down this formula:
-   The dot product $\mathbf{u} \cdot \mathbf{v}$ calculates a scalar value that is proportional to the length of the projection. Specifically, $\mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \|\mathbf{v}\| \cos(\theta)$, where $\theta$ is the angle between the two vectors.
-   $\|\mathbf{v}\|^2$ is the squared magnitude (length) of vector **v**.
-   The fraction $\frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{v}\|^2}$ is a scalar that scales the vector **v**. It represents the length of the projection of **u** onto the direction of **v**, normalized by the length of **v**.
-   Multiplying this scalar by the vector **v** gives us the projection vector, which has the same direction as **v**.

**Orthogonality** is the generalization of perpendicularity to higher dimensions. Two vectors **u** and **v** are said to be orthogonal if the angle between them is 90 degrees. A key property of orthogonal vectors is that their dot product is zero.

$$
\mathbf{u} \cdot \mathbf{v} = 0
$$

This makes sense geometrically because if $\theta = 90^\circ$, then $\cos(\theta) = 0$, making the dot product zero.

The concept of projection is deeply connected to orthogonality. If we define a vector **w** as the difference between **u** and its projection onto **v**, i.e., $\mathbf{w} = \mathbf{u} - \text{proj}_{\mathbf{v}}\mathbf{u}$, then this vector **w** is orthogonal to **v**. This vector **w** is often called the "error vector" or the "residual," as it represents the component of **u** that is "left over" after projecting **u** onto **v**. This forms the basis for decompositions like the Gram-Schmidt process, which creates an orthonormal basis from an arbitrary set of vectors.

<br>

## 2. Geometric Meaning of Least Squares

The method of **Least Squares** is a standard approach in regression analysis to approximate the solution of overdetermined systems of linear equations, i.e., systems where there are more equations than unknowns. Such a system can be written as $A\mathbf{x} = \mathbf{b}$, where $A$ is an $m \times n$ matrix with $m > n$.

Geometrically, the equation $A\mathbf{x} = \mathbf{b}$ asks if the vector **b** can be expressed as a linear combination of the column vectors of matrix $A$. In other words, is **b** in the column space of $A$ (denoted as $C(A)$)?

For an overdetermined system, the vector **b** typically does not lie in $C(A)$, meaning there is no exact solution $\mathbf{x}$ that satisfies the equation. The least squares method aims to find the "best possible" solution, which we'll call $\hat{\mathbf{x}}$. This "best" solution is the one that minimizes the Euclidean norm of the residual vector, $\|\mathbf{b} - A\hat{\mathbf{x}}\|$.

Geometrically, finding this minimal distance corresponds to finding the vector in the column space of $A$ that is closest to **b**. This closest vector is the orthogonal projection of **b** onto the column space $C(A)$. Let's call this projection $\mathbf{p}$. So, we are looking for a solution $\hat{\mathbf{x}}$ such that $A\hat{\mathbf{x}} = \mathbf{p}$.

The residual vector, $\mathbf{e} = \mathbf{b} - \mathbf{p} = \mathbf{b} - A\hat{\mathbf{x}}$, must be orthogonal to the column space $C(A)$. This means the residual vector must be orthogonal to every column vector of $A$. This orthogonality condition can be expressed as:

$$
A^T(\mathbf{b} - A\hat{\mathbf{x}}) = \mathbf{0}
$$

This leads to the famous **normal equations**:

$$
A^T A \hat{\mathbf{x}} = A^T \mathbf{b}
$$

By solving this system for $\hat{\mathbf{x}}$, we find the least squares solution. In essence, the least squares problem transforms an unsolvable system into a solvable one by projecting the target vector onto the subspace spanned by the input vectors, thereby finding the closest possible approximation within that subspace.

<br>

## 3. Basis and Change of Basis

A **basis** for a vector space is a set of linearly independent vectors that span the entire space. This means that any vector in the space can be uniquely expressed as a linear combination of the basis vectors. For example, the standard basis in $\mathbb{R}^2$ is composed of the vectors $\mathbf{e}_1 = [1, 0]^T$ and $\mathbf{e}_2 = [0, 1]^T$. Any vector $[x, y]^T$ can be written as $x\mathbf{e}_1 + y\mathbf{e}_2$.

While the standard basis is convenient, it's not always the most informative one. Data often has its own inherent structure that is not aligned with the standard axes. By choosing a different basis, we can often simplify the representation of the data and reveal important properties.

A **change of basis** is the process of expressing a vector in terms of a new set of basis vectors. Suppose we have a vector $\mathbf{v}$ and two different bases, $B = \{\mathbf{b}_1, \mathbf{b}_2, ..., \mathbf{b}_n\}$ and $C = \{\mathbf{c}_1, \mathbf{c}_2, ..., \mathbf{c}_n\}$. The coordinates of $\mathbf{v}$ in basis $B$ are $[\mathbf{v}]_B$ and in basis $C$ are $[\mathbf{v}]_C$. The change of basis is performed using a transformation matrix $P$, often called the change-of-basis matrix, such that:

$$
[\mathbf{v}]_C = P_{C \leftarrow B} [\mathbf{v}]_B
$$

The columns of the matrix $P_{C \leftarrow B}$ are the coordinates of the old basis vectors ($B$) expressed in terms of the new basis ($C$).