# [Lecture Note] Linear Algebra II - Lecture Notes

> Written by Hongseo Jang

## 1. Matrix Types and Core Operations

### Common Types of Matrices

* **Identity Matrix ($\mathbf{I}$):** A square matrix with ones on the main diagonal and zeros everywhere else. It is the multiplicative identity for matrices, meaning $\mathbf{A}\mathbf{I} = \mathbf{I}\mathbf{A} = \mathbf{A}$.
* **Symmetric Matrix:** A square matrix that is equal to its own transpose, i.e., $\mathbf{A} = \mathbf{A}^T$. The elements $a_{ij}$ are equal to $a_{ji}$. Covariance matrices are a common example.
* **Diagonal Matrix:** A matrix where all off-diagonal elements are zero. The identity matrix is a special case of a diagonal matrix.
* **Orthogonal Matrix:** A square matrix where its columns (and rows) are orthogonal unit vectors. A key property is that its transpose is equal to its inverse: $\mathbf{A}^T = \mathbf{A}^{-1}$. Linear transformations by orthogonal matrices preserve vector lengths and angles, representing pure rotations or reflections.

### Matrix Operations

* **Matrix Multiplication:** The product of two matrices $\mathbf{A}$ (of size $m \times n$) and $\mathbf{B}$ (of size $n \times p$) is a matrix $\mathbf{C}$ (of size $m \times p$). The element $C_{ij}$ is the dot product of the $i$-th row of $\mathbf{A}$ and the $j$-th column of $\mathbf{B}$.
    * **Important:** Matrix multiplication is **not** commutative; in general, $\mathbf{AB} \neq \mathbf{BA}$.

* **Inverse Matrix ($\mathbf{A}^{-1}$):** For a square matrix $\mathbf{A}$, its inverse $\mathbf{A}^{-1}$ is a matrix such that their product is the identity matrix:
    $$
    \mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}
    $$
    A matrix only has an inverse if it is **non-singular**, which means its determinant is non-zero.

<br>

## 2. Systems of Linear Equations

A system of linear equations, such as:
$a_{11}x_1 + a_{12}x_2 = b_1$
$a_{21}x_1 + a_{22}x_2 = b_2$

can be expressed compactly in matrix form as $\mathbf{A}\mathbf{x} = \mathbf{b}$, where:
$$
\mathbf{A} = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}, \quad \mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}
$$

If the matrix of coefficients $\mathbf{A}$ is invertible, we can find a unique solution for the vector of variables $\mathbf{x}$ by multiplying both sides by the inverse of $\mathbf{A}$:
$$
\mathbf{A}^{-1}\mathbf{A}\mathbf{x} = \mathbf{A}^{-1}\mathbf{b} \implies \mathbf{I}\mathbf{x} = \mathbf{A}^{-1}\mathbf{b} \implies \mathbf{x} = \mathbf{A}^{-1}\mathbf{b}
$$
This is a powerful method for solving systems of equations that appears frequently in science and engineering.

<br>

## 3. Linear Transformations and Determinants

### Linear Transformation

A transformation $T$ is a function that maps vectors from one vector space to another. It is a **linear transformation** if it preserves the operations of vector addition and scalar multiplication. Formally, for any vectors $\mathbf{v}, \mathbf{w}$ and scalars $c, d$:

$$
T(c\mathbf{v} + d\mathbf{w}) = cT(\mathbf{v}) + dT(\mathbf{w})
$$

This property implies that grid lines in the original space remain parallel and evenly spaced after the transformation. Any linear transformation can be represented by a matrix multiplication. That is, applying a transformation $T$ to a vector $\mathbf{v}$ is equivalent to multiplying $\mathbf{v}$ by a matrix $\mathbf{A}$, i.e., $T(\mathbf{v}) = \mathbf{A}\mathbf{v}$.

### Determinant

The determinant is a scalar value, denoted $\det(\mathbf{A})$ or $|\mathbf{A}|$, that can be computed from the elements of a square matrix.

* **Geometric Meaning:** The determinant tells us about the change in area (in 2D) or volume (in 3D) after applying the linear transformation associated with the matrix.
    * The **absolute value** $|\det(\mathbf{A})|$ is the scaling factor. If we take a unit square (area 1) and transform it using matrix $\mathbf{A}$, the area of the resulting parallelogram will be $|\det(\mathbf{A})|$.
    * The **sign** of the determinant indicates whether the transformation flips the orientation of the space. A positive determinant means the orientation is preserved, while a negative determinant means it has been inverted (like a reflection).
    * A determinant of **zero** means the transformation squashes the space into a lower dimension (e.g., a plane becomes a line). This is why matrices with $\det(\mathbf{A})=0$ are singular and have no inverseâ€”the transformation is irreversible because information is lost.

<br>

## 4. Application in ML: Linear Regression

Linear algebra provides the language to concisely express and solve complex models like linear regression. The model, which predicts a target value $y$ based on a set of features $x_i$, can be written for a whole dataset at once.

The model for all $n$ observations can be written as:

$$
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
$$

Let's break down each component:
* $\mathbf{y}$ is an $n \times 1$ vector of the observed target values ($y_1, y_2, \dots, y_n$).
* $\mathbf{X}$ is the $n \times (p+1)$ **design matrix**, where $n$ is the number of samples and $p$ is the number of features. Each row is a single data sample. A column of ones is typically prepended to handle the intercept term.
    $$
    \mathbf{X} = \begin{pmatrix} 1 & x_{11} & \dots & x_{1p} \\ 1 & x_{21} & \dots & x_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n1} & \dots & x_{np} \end{pmatrix}
    $$
* $\boldsymbol{\beta}$ is a $(p+1) \times 1$ vector containing the model's coefficients ($\beta_0, \beta_1, \dots, \beta_p$) that we need to learn from the data.
* $\boldsymbol{\epsilon}$ is an $n \times 1$ vector of random error terms.