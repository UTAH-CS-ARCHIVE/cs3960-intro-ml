# [Lecture Note] Optimization Theory

> Written by Hongseo Jang

#### **1. The Limitations of Basic Gradient Descent**

In our exploration of optimization, we began with the foundational algorithm of Gradient Descent (GD). The update rule is simple and intuitive: compute the gradient of the loss function $J(\boldsymbol{\theta})$ with respect to all parameters $\boldsymbol{\theta}$ using the entire training dataset, and then take a step in the opposite direction.

$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J(\boldsymbol{\theta}_t)
$$

While this guarantees a move towards a minimum, it suffers from major drawbacks in the context of modern machine learning:
* **Computational Cost:** For datasets with millions of samples, computing the gradient over the entire dataset for a single parameter update is prohibitively slow and memory-intensive.
* **Local Minima & Saddle Points:** It can easily get trapped in poor local minima or slow down drastically on the flat surfaces of saddle points, which are common in high-dimensional loss landscapes.
* **Ravines:** On surfaces with long, narrow valleys (ravines), the gradient points steeply across the valley rather than down its length. This causes the optimizer to oscillate back and forth, making very slow progress towards the actual minimum.

These limitations necessitate more sophisticated optimization algorithms.

<br>

#### **2. Stochastic Gradient Descent (SGD)**

Stochastic Gradient Descent (SGD) addresses the computational cost of GD head-on. Instead of using the entire dataset, SGD approximates the gradient using a single, randomly chosen data point $(\mathbf{x}^{(i)}, y^{(i)})$ at each step.

$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J(\boldsymbol{\theta}_t; \mathbf{x}^{(i)}, y^{(i)})
$$

A common and more stable variant is **Mini-batch SGD**, which uses a small, random subset of the data (a "mini-batch") to compute the gradient. This is the de-facto standard in deep learning.

* **Advantages:**
    * **Speed:** Updates are far more frequent and computationally cheaper.
    * **Escaping Minima:** The inherent noise in the gradient estimate can help the optimizer "jump out" of sharp, shallow local minima and better navigate saddle points.
* **Disadvantages:**
    * **High Variance:** The path to the minimum is not smooth. The loss function fluctuates significantly due to the noisy gradient estimates, which can make convergence difficult.

<br>

#### **3. Momentum: Adding Inertia to the Update**

The Momentum method was developed to accelerate SGD and dampen its oscillations. The core idea is to introduce a "velocity" vector $\mathbf{v}$ that accumulates an exponentially decaying moving average of past gradients. This gives the update step inertia, like a ball rolling down a hill.

The update rules are:

$$
\mathbf{v}_{t+1} = \gamma \mathbf{v}_t + \eta \nabla J(\boldsymbol{\theta}_t)
$$

$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \mathbf{v}_{t+1}
$$

Here:
* $\mathbf{v}_t$ is the velocity vector at time step $t$.
* $\gamma$ is the momentum coefficient (e.g., 0.9), which determines how much of the past velocity is carried over. It acts like friction.
* $\eta \nabla J(\boldsymbol{\theta}_t)$ is the current gradient's contribution, acting like an acceleration.

**How it works:**
* **Acceleration:** In directions where the gradient consistently points the same way, the velocity term builds up, causing the optimizer to move faster.
* **Dampening:** In directions where the gradient oscillates (like across a ravine), the successive positive and negative gradient contributions cancel each other out in the velocity vector, dampening the oscillations and keeping the optimizer on a more direct path.

<br>

#### **4. Adam: Adaptive Moment Estimation**

Adam is arguably the most popular and effective optimization algorithm for deep learning today. It combines the idea of **momentum** with the concept of **adaptive learning rates**, where each parameter gets its own individual learning rate. It computes adaptive learning rates for each parameter by keeping track of two exponentially decaying moving averages.

1.  **First Moment (Mean):** A moving average of the gradients, which is essentially the momentum term.
    $$
    \mathbf{m}_{t} = \beta_1 \mathbf{m}_{t-1} + (1-\beta_1) \nabla J(\boldsymbol{\theta}_t)
    $$
2.  **Second Moment (Uncentered Variance):** A moving average of the squared gradients. This is used to scale the learning rate for each parameter.
    $$
    \mathbf{v}_{t} = \beta_2 \mathbf{v}_{t-1} + (1-\beta_2) (\nabla J(\boldsymbol{\theta}_t))^2
    $$

Since these moving averages are initialized at zero, they are biased towards zero, especially at the beginning of training. Adam corrects for this bias:

$$
\hat{\mathbf{m}}_t = \frac{\mathbf{m}_t}{1 - \beta_1^t}
$$

$$
\hat{\mathbf{v}}_t = \frac{\mathbf{v}_t}{1 - \beta_2^t}
$$

Finally, the parameter update rule for Adam is:

$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \frac{\eta}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon} \hat{\mathbf{m}}_t
$$

Here, $\beta_1$ and $\beta_2$ are decay rates (e.g., 0.9 and 0.999), and $\epsilon$ is a small constant to prevent division by zero. The update is essentially the momentum-like term $\hat{\mathbf{m}}_t$ scaled by a per-parameter learning rate that is inversely proportional to the square root of its past squared gradients ($\sqrt{\hat{\mathbf{v}}_t}$).

Adam combines the best of both worlds: the accelerated, stable steps from momentum and the per-parameter learning rate adaptation from methods like RMSProp, making it a robust and powerful default choice for training deep neural networks.