# [Lecture Note] Probability & Statistics I

> Written by Hongseo Jang

#### **1. Introduction: A Tale of Two Approaches**

In machine learning, classification models can be broadly categorized into two families: generative and discriminative. At a high level, the distinction is about what they learn from the data.

* A **generative model** learns the underlying probability distribution of the data for each class. It tries to understand "how the data is generated."
* A **discriminative model** learns the boundary between different classes directly. It focuses solely on the task of separating the data points.

Imagine the task of distinguishing between handwritten digits '0' and '1'.
* A generative model would learn what a '0' looks like and what a '1' looks like. With this knowledge, it could even *generate* a new image of a '0' or a '1'. To classify a new image, it checks whether it looks more like its learned concept of a '0' or a '1'.
* A discriminative model isn't concerned with what the digits look like fundamentally. It only learns the specific features and differences that are most useful for telling them apart. It finds a line or a curve (a decision boundary) that best separates the '0's from the '1's in the feature space.

<br>

#### **2. The Probabilistic Viewpoint**

The core difference between these two approaches lies in the probability distributions they model. Let $\mathbf{x}$ be the input features and $y$ be the class label.

##### **Generative Models**

Generative models learn the **joint probability distribution** $P(\mathbf{x}, y)$. This distribution describes the probability of observing a feature vector $\mathbf{x}$ and a label $y$ together. Using the chain rule of probability, the joint probability can be factored in two ways:

$$
P(\mathbf{x}, y) = P(\mathbf{x}|y)P(y)
$$

or

$$
P(\mathbf{x}, y) = P(y|\mathbf{x})P(\mathbf{x})
$$

Generative models focus on the first factorization. They model:
1.  **The class prior $P(y)$:** The overall probability of a class, irrespective of the features.
2.  **The class-conditional probability (likelihood) $P(\mathbf{x}|y)$:** The probability of observing feature vector $\mathbf{x}$ given that it belongs to class $y$.

For classification of a new input $\mathbf{x}$, we want to find the class $y$ that maximizes the posterior probability $P(y|\mathbf{x})$. We can get this using Bayes' theorem:

$$
P(y|\mathbf{x}) = \frac{P(\mathbf{x}|y)P(y)}{P(\mathbf{x})}
$$

Since $P(\mathbf{x})$ is a constant for a given input, the decision rule is:

$$
\hat{y} = \arg\max_{y} P(\mathbf{x}|y)P(y)
$$

By modeling the joint distribution, generative models can be used for tasks other than classification, such as generating new samples $(\mathbf{x}, y)$.

<br>

##### **Discriminative Models**

Discriminative models take a more direct approach. They bypass the modeling of the joint distribution and directly learn the **conditional probability distribution** $P(y|\mathbf{x})$.

$$
\hat{y} = \arg\max_{y} P(y|\mathbf{x})
$$

These models directly map the input features $\mathbf{x}$ to a class label $y$. They learn the decision boundary that separates the classes without making any assumptions about the underlying distribution of the features $\mathbf{x}$. This focused approach often leads to better performance on pure classification tasks, especially when the assumptions made by a generative model about the data distribution do not hold true.

<br>

#### **3. Case Study: Naive Bayes (A Generative Model)**

The Naive Bayes classifier is a classic example of a generative model. As derived above, its goal is to find the class $y$ that maximizes $P(\mathbf{x}|y)P(y)$.

It explicitly models:
* The prior $P(y)$, which is usually estimated from the frequency of each class in the training data.
* The likelihood $P(\mathbf{x}|y)$. This is where the "naive" assumption comes into play. It assumes that all features are conditionally independent given the class:

$$
P(\mathbf{x}|y) = P(x_1, x_2, \dots, x_p|y) = \prod_{i=1}^{p} P(x_i|y)
$$

By modeling both $P(y)$ and $P(\mathbf{x}|y)$, Naive Bayes learns a model of the joint distribution $P(\mathbf{x}, y)$, fitting the definition of a generative model. For instance, in Gaussian Naive Bayes, we assume that $P(x_i|y)$ follows a Gaussian distribution and we learn the mean and variance for each feature $i$ in each class $y$.

<br>

#### **4. Comparison: Naive Bayes vs. Logistic Regression**

Logistic Regression is a quintessential discriminative model. It provides a good contrast to Naive Bayes.

**Modeling:**
* **Naive Bayes (Generative):** Models $P(\mathbf{x}|y)$ and $P(y)$. It learns the distribution of the data itself. The decision boundary is a byproduct of the modeled distributions.
* **Logistic Regression (Discriminative):** Directly models $P(y|\mathbf{x})$ using the logistic (sigmoid) function. For a binary classification problem:

$$
P(y=1|\mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x} + b)}}
$$

    It focuses only on finding the parameters $\mathbf{w}$ and $b$ that best define the decision boundary, without caring about the distribution of $\mathbf{x}$.

**Assumptions & Performance:**
* **Assumptions:** Naive Bayes makes a strong, "naive" assumption of feature independence. Logistic Regression makes no such assumption about the features.
* **Data Requirement:** Because of its strong assumptions, Naive Bayes can converge to a reasonable model with very little training data. Logistic Regression, being more flexible, often requires more data to outperform Naive Bayes but typically achieves higher accuracy if the feature independence assumption is violated.
* **Decision Boundary:** Logistic Regression always finds a linear decision boundary. The decision boundary for Gaussian Naive Bayes is also linear if the covariance matrices for each class are assumed to be identical, but it becomes quadratic otherwise.

In summary, the choice between a generative and a discriminative model involves a trade-off. Generative models make stronger assumptions about the data, which can be a powerful advantage when data is scarce. Discriminative models are more flexible and often yield superior results for classification tasks, as they are trained directly for that specific purpose.