# [Lecture Note] Introduction & Linear Algebra I - Lecture Notes

> Written by Hongseo Jang

## 1. Introduction to Machine Learning

* **Definition:** Machine Learning (ML) is a subfield of artificial intelligence. Arthur Samuel, a pioneer in the field, defined it in 1959 as "the field of study that gives computers the ability to learn without being explicitly programmed." In essence, ML is about developing algorithms that can identify patterns and make decisions from data, improving their performance with experience.

* **A Brief History:**
    * **1950s:** The field began with early models like the Perceptron, which is a simple algorithm for binary classification.
    * **1970s-80s:** Progress slowed down during the first "AI winter" due to computational limits and high expectations. However, this period also saw the development of key ideas like backpropagation, which would become crucial later.
    * **1990s:** Machine learning became more data-driven, with a focus on statistical methods. Algorithms like Support Vector Machines (SVMs) gained popularity.
    * **2010s-Present:** The modern "deep learning" era. The combination of massive datasets ("Big Data"), powerful hardware (especially GPUs), and algorithmic refinements has led to breakthroughs in areas like image recognition, natural language processing, and more.

* **Why is Math Important?** Math is the foundational language of machine learning. Understanding linear algebra, calculus, probability, and statistics is essential for moving beyond simply using ML libraries as "black boxes." It allows us to understand how algorithms work, why they succeed or fail, and how to design new, improved methods.

<br>

## 2. Core Data Structures: Scalars, Vectors, Matrices, Tensors

These are the fundamental objects we use to store and manipulate data in linear algebra.

* **Scalar:** A single number (e.g., $x = 5$). It's a 0-dimensional tensor.
* **Vector:** An ordered array of numbers. A vector can be thought of as a point in a multi-dimensional space. It's a 1-dimensional tensor.
    * Example: A vector representing a house might be $v = [\text{number of bedrooms, square footage, age}]$.
* **Matrix:** A 2D array of numbers, with rows and columns. A matrix can represent an entire dataset, where rows are individual samples and columns are features. It's a 2-dimensional tensor.
* **Tensor:** The generalization of these concepts to *n* dimensions. While the term can technically refer to scalars, vectors, and matrices, it's often used for arrays with 3 or more dimensions.
    * Example: A color image can be a 3D tensor: (height, width, color channels).

<br>

## 3. Vector Spaces and Data Representation

* **Vector Space:** A collection of vectors where two operations are well-defined: vector addition and scalar multiplication. The space must be closed under these operations (meaning the result of the operation is still within the space). For example, $\mathbb{R}^n$ is the vector space containing all n-dimensional vectors with real-valued components.
* **Subspace:** A subset of a larger vector space that is itself a vector space. It must contain the zero vector.
* **Vector Representation of Data:** A crucial step in any ML pipeline is converting raw data into numerical vectors, a process called **feature extraction** or **vectorization**. For example, text data can be converted into vectors using methods like Bag-of-Words (where each vector element is the count of a word in a vocabulary) or more advanced techniques like Word2Vec. This allows us to use the mathematical tools of linear algebra to analyze and find patterns in the data.

<br>

## 4. Key Mathematical Concepts

### Vector Dot Product

The dot product (or inner product) of two vectors $a$ and $b$ in $\mathbb{R}^n$ is defined as:
$$
a \cdot b = \sum_{i=1}^{n} a_i b_i = a_1b_1 + a_2b_2 + \dots + a_nb_n
$$

* **Geometric Meaning & Similarity:** The dot product has a powerful geometric interpretation:
    $$
    a \cdot b = \|a\|_2 \|b\|_2 \cos(\theta)
    $$
    where $\theta$ is the angle between the two vectors. We can rearrange this to find the angle itself:
    $$
    \cos(\theta) = \frac{a \cdot b}{\|a\|_2 \|b\|_2}
    $$
    This formula is the basis for **Cosine Similarity**. It measures the orientation of two vectors, irrespective of their magnitude.
    * If $\cos(\theta) = 1$, the vectors point in the same direction (maximum similarity).
    * If $\cos(\theta) = 0$, the vectors are orthogonal (no similarity).
    * If $\cos(\theta) = -1$, the vectors point in opposite directions (maximum dissimilarity).
    This is widely used in NLP to measure how similar documents or words are based on their vector representations.

### Vector Norms

A norm is a function that assigns a length or size to a vector.

* **L1 Norm (Manhattan Norm):**
    The L1 norm is the sum of the absolute values of the components.
    $$
    \|x\|_1 = \sum_{i=1}^{n} |x_i|
    $$
    It's called the Manhattan norm because it represents the distance you would travel between two points in a city grid, moving only along horizontal or vertical streets. In ML, the L1 norm is used in regularization (e.g., LASSO regression) as it tends to produce sparse solutions (i.e., it drives many model parameters to zero).

* **L2 Norm (Euclidean Norm):**
    The L2 norm is the most common norm, representing the standard Euclidean distance from the origin to the point defined by the vector.
    $$
    \|x\|_2 = \sqrt{\sum_{i=1}^{n} x_i^2}
    $$
    The squared L2 norm ($ \|x\|_2^2 = \sum_{i=1}^{n} x_i^2 $) is often used for mathematical convenience as it removes the square root. The L2 norm is used in Ridge regression for regularization and is the basis for the Euclidean distance between two vectors, calculated as $\|a - b\|_2$.