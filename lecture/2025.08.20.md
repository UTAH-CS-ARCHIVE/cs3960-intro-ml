## [Lecture Note] Linear Regression: Finding Intercept and Weights using Partial Derivatives

### 1. Simple Linear Regression
We start with the model:

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

where:
- $y_i$: dependent variable  
- $x_i$: independent variable  
- $\beta_0$: intercept  
- $\beta_1$: slope (weight)  
- $\epsilon_i$: error term  

The objective is to minimize the **sum of squared errors (SSE)**:

$$
S(\beta_0, \beta_1) = \sum_{i=1}^n \big(y_i - (\beta_0 + \beta_1 x_i)\big)^2
$$

#### Partial Derivatives
To find the optimal parameters, we take partial derivatives and set them equal to zero.

1. With respect to $\beta_0$:

$$
\frac{\partial S}{\partial \beta_0} 
= -2 \sum_{i=1}^n \big(y_i - \beta_0 - \beta_1 x_i\big) = 0
$$

$$
\Rightarrow \sum_{i=1}^n y_i = n\beta_0 + \beta_1 \sum_{i=1}^n x_i
$$

2. With respect to $\beta_1$:

$$
\frac{\partial S}{\partial \beta_1} 
= -2 \sum_{i=1}^n x_i \big(y_i - \beta_0 - \beta_1 x_i\big) = 0
$$

$$
\Rightarrow \sum_{i=1}^n x_i y_i = \beta_0 \sum_{i=1}^n x_i + \beta_1 \sum_{i=1}^n x_i^2
$$

#### Solving the Normal Equations
We now have two equations:

$$
\sum y_i = n\beta_0 + \beta_1 \sum x_i
$$

$$
\sum x_i y_i = \beta_0 \sum x_i + \beta_1 \sum x_i^2
$$

From these, we solve for:

$$
\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
$$

$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
$$

where $\bar{x}$, $\bar{y}$ are sample means.

---

### 2. Multiple Regression and Statistical Testing

In multiple regression:

$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i
$$

The parameters are estimated via:

$$
\hat{\boldsymbol{\beta}} = (X^T X)^{-1} X^T \mathbf{y}
$$

where $X$ is the design matrix.

#### Hypothesis Testing for Each Coefficient
We test:

$$
H_0: \beta_j = 0 \quad \text{vs} \quad H_1: \beta_j \neq 0
$$

The estimated variance of $\hat{\beta}_j$ is:

$$
\text{Var}(\hat{\beta}_j) = \hat{\sigma}^2 \cdot (X^T X)^{-1}_{jj}
$$

where

$$
\hat{\sigma}^2 = \frac{1}{n-p-1} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

The **t-statistic** is:

$$
t_j = \frac{\hat{\beta}_j}{\sqrt{\text{Var}(\hat{\beta}_j)}}
$$

---

### 3. The Meaning of the t-Distribution

The \(t\)-distribution arises when standardizing a normal variable with an estimated variance.

If:

$$
Z \sim N(0,1), \quad U \sim \chi^2_\nu \quad \text{independent}
$$

then:

$$
T = \frac{Z}{\sqrt{U / \nu}} \sim t_\nu
$$

In regression:
- Numerator: estimate $\hat{\beta}_j$ centered at 0 under $H_0$ 
- Denominator: standard error estimated from residuals  
- Degrees of freedom: $\nu = n - p - 1$

Thus, the **t-distribution** accounts for the extra uncertainty from estimating variance.  
Large absolute $t_j$ → reject $H_0$ → variable $x_j$ is statistically significant.