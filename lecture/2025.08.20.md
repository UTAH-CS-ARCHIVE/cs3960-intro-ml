# [Lecture Note] Introduction & Linear Algebra I

> Written by Hongseo Jang

## 1. Introduction to Machine Learning

* **Definition:** Machine Learning (ML) is a subfield of artificial intelligence. Arthur Samuel, a pioneer in the field, defined it in 1959 as "the field of study that gives computers the ability to learn without being explicitly programmed." In essence, ML is about developing algorithms that can identify patterns and make decisions from data, improving their performance with experience.

* **A Brief History:**
    * **1950s:** The field began with early models like the Perceptron, which is a simple algorithm for binary classification.
    * **1970s-80s:** Progress slowed down during the first "AI winter" due to computational limits and high expectations. However, this period also saw the development of key ideas like backpropagation, which would become crucial later.
    * **1990s:** Machine learning became more data-driven, with a focus on statistical methods. Algorithms like Support Vector Machines (SVMs) gained popularity.
    * **2010s-Present:** The modern "deep learning" era. The combination of massive datasets ("Big Data"), powerful hardware (especially GPUs), and algorithmic refinements has led to breakthroughs in areas like image recognition, natural language processing, and more.

* **Why is Math Important?** Math is the foundational language of machine learning. Understanding linear algebra, calculus, probability, and statistics is essential for moving beyond simply using ML libraries as "black boxes." It allows us to understand how algorithms work, why they succeed or fail, and how to design new, improved methods.

<br>

## 2. Core Data Structures: Scalars, Vectors, Matrices, Tensors

These are the fundamental objects we use to store and manipulate data in linear algebra.

* **Scalar:** A single number (e.g., $x = 5$). It's a 0-dimensional tensor.
* **Vector:** An ordered array of numbers. A vector can be thought of as a point in a multi-dimensional space. It's a 1-dimensional tensor.
    * Example: A vector representing a house might be $v = [\text{number of bedrooms, square footage, age}]$.
* **Matrix:** A 2D array of numbers, with rows and columns. A matrix can represent an entire dataset, where rows are individual samples and columns are features. It's a 2-dimensional tensor.
* **Tensor:** The generalization of these concepts to *n* dimensions. While the term can technically refer to scalars, vectors, and matrices, it's often used for arrays with 3 or more dimensions.
    * Example: A color image can be a 3D tensor: (height, width, color channels).

<br>

## 3. Vector Spaces and Data Representation

* **Vector Space:** A collection of vectors where two operations are well-defined: vector addition and scalar multiplication. The space must be closed under these operations (meaning the result of the operation is still within the space). For example, $\mathbb{R}^n$ is the vector space containing all n-dimensional vectors with real-valued components.
* **Subspace:** A subset of a larger vector space that is itself a vector space. It must contain the zero vector.
* **Vector Representation of Data:** A crucial step in any ML pipeline is converting raw data into numerical vectors, a process called **feature extraction** or **vectorization**. For example, text data can be converted into vectors using methods like Bag-of-Words (where each vector element is the count of a word in a vocabulary) or more advanced techniques like Word2Vec. This allows us to use the mathematical tools of linear algebra to analyze and find patterns in the data.

<br>

## 4. Key Mathematical Concepts

### Vector Dot Product

The dot product (or inner product) of two vectors $a$ and $b$ in $\mathbb{R}^n$ is defined as:
$$
a \cdot b = \sum_{i=1}^{n} a_i b_i = a_1b_1 + a_2b_2 + \dots + a_nb_n
$$

* **Geometric Meaning & Similarity:** The dot product has a powerful geometric interpretation:
    $$
    a \cdot b = \|a\|_2 \|b\|_2 \cos(\theta)
    $$
    where $\theta$ is the angle between the two vectors. We can rearrange this to find the angle itself:
    $$
    \cos(\theta) = \frac{a \cdot b}{\|a\|_2 \|b\|_2}
    $$
    This formula is the basis for **Cosine Similarity**. It measures the orientation of two vectors, irrespective of their magnitude.
    * If $\cos(\theta) = 1$, the vectors point in the same direction (maximum similarity).
    * If $\cos(\theta) = 0$, the vectors are orthogonal (no similarity).
    * If $\cos(\theta) = -1$, the vectors point in opposite directions (maximum dissimilarity).
    This is widely used in NLP to measure how similar documents or words are based on their vector representations.

### Vector Norms

A norm is a function that assigns a length or size to a vector.

* **L1 Norm (Manhattan Norm):**
    The L1 norm is the sum of the absolute values of the components.
    $$
    \|x\|_1 = \sum_{i=1}^{n} |x_i|
    $$
    It's called the Manhattan norm because it represents the distance you would travel between two points in a city grid, moving only along horizontal or vertical streets. In ML, the L1 norm is used in regularization (e.g., LASSO regression) as it tends to produce sparse solutions (i.e., it drives many model parameters to zero).

* **L2 Norm (Euclidean Norm):**
    The L2 norm is the most common norm, representing the standard Euclidean distance from the origin to the point defined by the vector.
    $$
    \|x\|_2 = \sqrt{\sum_{i=1}^{n} x_i^2}
    $$
    The squared L2 norm ($ \|x\|_2^2 = \sum_{i=1}^{n} x_i^2 $) is often used for mathematical convenience as it removes the square root. The L2 norm is used in Ridge regression for regularization and is the basis for the Euclidean distance between two vectors, calculated as $\|a - b\|_2$.

<br>

## 5. Projection and Orthogonality

**Projection** is a fundamental concept that allows us to find the component of one vector that lies in the direction of another. Imagine shining a light from directly above onto a vector **u**. The shadow it casts on the line defined by vector **v** is the projection of **u** onto **v**.

The formula to calculate the projection of vector **u** onto vector **v** is given by:

$$
\text{proj}_{\mathbf{v}}\mathbf{u} = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{v}\|^2}\mathbf{v}
$$

Let's break down this formula:
-   The dot product $\mathbf{u} \cdot \mathbf{v}$ calculates a scalar value that is proportional to the length of the projection. Specifically, $\mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \|\mathbf{v}\| \cos(\theta)$, where $\theta$ is the angle between the two vectors.
-   $\|\mathbf{v}\|^2$ is the squared magnitude (length) of vector **v**.
-   The fraction $\frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{v}\|^2}$ is a scalar that scales the vector **v**. It represents the length of the projection of **u** onto the direction of **v**, normalized by the length of **v**.
-   Multiplying this scalar by the vector **v** gives us the projection vector, which has the same direction as **v**.

**Orthogonality** is the generalization of perpendicularity to higher dimensions. Two vectors **u** and **v** are said to be orthogonal if the angle between them is 90 degrees. A key property of orthogonal vectors is that their dot product is zero.

$$
\mathbf{u} \cdot \mathbf{v} = 0
$$

This makes sense geometrically because if $\theta = 90^\circ$, then $\cos(\theta) = 0$, making the dot product zero.

The concept of projection is deeply connected to orthogonality. If we define a vector **w** as the difference between **u** and its projection onto **v**, i.e., $\mathbf{w} = \mathbf{u} - \text{proj}_{\mathbf{v}}\mathbf{u}$, then this vector **w** is orthogonal to **v**. This vector **w** is often called the "error vector" or the "residual," as it represents the component of **u** that is "left over" after projecting **u** onto **v**. This forms the basis for decompositions like the Gram-Schmidt process, which creates an orthonormal basis from an arbitrary set of vectors.

<br>

## 6. Geometric Meaning of Least Squares

The method of **Least Squares** is a standard approach in regression analysis to approximate the solution of overdetermined systems of linear equations, i.e., systems where there are more equations than unknowns. Such a system can be written as $A\mathbf{x} = \mathbf{b}$, where $A$ is an $m \times n$ matrix with $m > n$.

Geometrically, the equation $A\mathbf{x} = \mathbf{b}$ asks if the vector **b** can be expressed as a linear combination of the column vectors of matrix $A$. In other words, is **b** in the column space of $A$ (denoted as $C(A)$)?

For an overdetermined system, the vector **b** typically does not lie in $C(A)$, meaning there is no exact solution $\mathbf{x}$ that satisfies the equation. The least squares method aims to find the "best possible" solution, which we'll call $\hat{\mathbf{x}}$. This "best" solution is the one that minimizes the Euclidean norm of the residual vector, $\|\mathbf{b} - A\hat{\mathbf{x}}\|$.

Geometrically, finding this minimal distance corresponds to finding the vector in the column space of $A$ that is closest to **b**. This closest vector is the orthogonal projection of **b** onto the column space $C(A)$. Let's call this projection $\mathbf{p}$. So, we are looking for a solution $\hat{\mathbf{x}}$ such that $A\hat{\mathbf{x}} = \mathbf{p}$.

The residual vector, $\mathbf{e} = \mathbf{b} - \mathbf{p} = \mathbf{b} - A\hat{\mathbf{x}}$, must be orthogonal to the column space $C(A)$. This means the residual vector must be orthogonal to every column vector of $A$. This orthogonality condition can be expressed as:

$$
A^T(\mathbf{b} - A\hat{\mathbf{x}}) = \mathbf{0}
$$

This leads to the famous **normal equations**:

$$
A^T A \hat{\mathbf{x}} = A^T \mathbf{b}
$$

By solving this system for $\hat{\mathbf{x}}$, we find the least squares solution. In essence, the least squares problem transforms an unsolvable system into a solvable one by projecting the target vector onto the subspace spanned by the input vectors, thereby finding the closest possible approximation within that subspace.

<br>

## 7. Basis and Change of Basis

A **basis** for a vector space is a set of linearly independent vectors that span the entire space. This means that any vector in the space can be uniquely expressed as a linear combination of the basis vectors. For example, the standard basis in $\mathbb{R}^2$ is composed of the vectors $\mathbf{e}_1 = [1, 0]^T$ and $\mathbf{e}_2 = [0, 1]^T$. Any vector $[x, y]^T$ can be written as $x\mathbf{e}_1 + y\mathbf{e}_2$.

While the standard basis is convenient, it's not always the most informative one. Data often has its own inherent structure that is not aligned with the standard axes. By choosing a different basis, we can often simplify the representation of the data and reveal important properties.

A **change of basis** is the process of expressing a vector in terms of a new set of basis vectors. Suppose we have a vector $\mathbf{v}$ and two different bases, $B = \{\mathbf{b}_1, \mathbf{b}_2, ..., \mathbf{b}_n\}$ and $C = \{\mathbf{c}_1, \mathbf{c}_2, ..., \mathbf{c}_n\}$. The coordinates of $\mathbf{v}$ in basis $B$ are $[\mathbf{v}]_B$ and in basis $C$ are $[\mathbf{v}]_C$. The change of basis is performed using a transformation matrix $P$, often called the change-of-basis matrix, such that:

$$
[\mathbf{v}]_C = P_{C \leftarrow B} [\mathbf{v}]_B
$$

The columns of the matrix $P_{C \leftarrow B}$ are the coordinates of the old basis vectors ($B$) expressed in terms of the new basis ($C$).