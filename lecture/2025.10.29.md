# [Lecture Note] Linear Regression

> Written by Hongseo Jang

#### **1. The Problem of Overfitting**

In machine learning, a common pitfall is **overfitting**. This occurs when a model learns the training data *too well*, capturing not only the underlying patterns but also the noise and random fluctuations specific to that dataset. An overfitted model performs exceptionally well on the data it was trained on but fails to generalize to new, unseen data.

Imagine a student who crams for an exam by memorizing the exact answers to a practice test. They might ace the practice test, but if the real exam has slightly different questions, they will perform poorly because they didn't learn the underlying concepts. An overfitted model is like this student; it has "memorized" the data instead of "learning" the general principles. Regularization is a set of techniques designed to prevent this by discouraging the model from becoming too complex.

<br>

#### **2. Regularization via Cost Function Penalties**

The core idea of regularization is to modify the model's cost function, which we are trying to minimize. We add a **penalty term** that depends on the magnitude of the model's parameters (weights). This new cost function balances two goals:
1.  Fitting the training data well (the original loss term, e.g., Mean Squared Error).
2.  Keeping the model parameters small (the new penalty term).

A model with large weights can create a very complex decision boundary that fits the noise in the data. By penalizing large weights, we encourage the model to find a simpler solution that is more likely to generalize.

<br>

#### **3. Ridge Regression (L2 Regularization)**

Ridge Regression adds a penalty proportional to the **squared magnitude of the weights**. This is also known as L2 regularization. The cost function is:

$$
J(\boldsymbol{\theta}) = \text{MSE}(\boldsymbol{\theta}) + \alpha \|\boldsymbol{\theta}\|_2^2
$$

Where:
* $\text{MSE}(\boldsymbol{\theta})$ is the Mean Squared Error, the original loss function.
* $\|\boldsymbol{\theta}\|_2^2 = \sum_{j=1}^{p} \theta_j^2$ is the squared L2-norm (Euclidean norm) of the parameter vector. This is the sum of the squares of all the weights.
* $\alpha$ is the regularization hyperparameter. It controls the strength of the penalty. A larger $\alpha$ results in smaller weights and a simpler model.

The effect of the L2 penalty is to **shrink** the weights towards zero. It keeps the weights small and distributed, but it will not force them to be *exactly* zero (unless $\alpha$ is infinite).

<br>

#### **4. Lasso Regression (L1 Regularization)**

Lasso (Least Absolute Shrinkage and Selection Operator) Regression adds a penalty proportional to the **absolute value of the weights**. This is also known as L1 regularization. The cost function is:

$$
J(\boldsymbol{\theta}) = \text{MSE}(\boldsymbol{\theta}) + \alpha \|\boldsymbol{\theta}\|_1
$$

Where:
* $\|\boldsymbol{\theta}\|_1 = \sum_{j=1}^{p} |\theta_j|$ is the L1-norm (Manhattan norm) of the parameter vector.

The L1 penalty has a crucial difference from L2: it can shrink some parameter weights to be **exactly zero**. This means Lasso can perform **automatic feature selection**, effectively identifying and removing irrelevant or redundant features from the model by setting their corresponding weights to zero. This property makes Lasso very powerful in high-dimensional settings where many features might be useless.

<br>

#### **5. A Deeper View: The Bayesian Interpretation**

Regularization can be understood more deeply from a Bayesian perspective, specifically through **Maximum A Posteriori (MAP)** estimation.

First, recall that standard unregularized regression (minimizing MSE) is equivalent to finding the **Maximum Likelihood Estimate (MLE)** of the parameters. MLE seeks the parameters $\boldsymbol{\theta}$ that maximize the likelihood of observing the data, $P(\text{Data}|\boldsymbol{\theta})$.

MAP estimation goes a step further by incorporating a **prior probability distribution** $P(\boldsymbol{\theta})$, which represents our prior belief about what the parameters should look like before we've seen any data. MAP seeks to maximize the *posterior* probability:

$$
\boldsymbol{\theta}_{\text{MAP}} = \arg\max_{\boldsymbol{\theta}} P(\boldsymbol{\theta}|\text{Data})
$$

Using Bayes' theorem, $P(\boldsymbol{\theta}|\text{Data}) \propto P(\text{Data}|\boldsymbol{\theta})P(\boldsymbol{\theta})$. To maximize this, we can maximize its logarithm:

$$
\boldsymbol{\theta}_{\text{MAP}} = \arg\max_{\boldsymbol{\theta}} [\log P(\text{Data}|\boldsymbol{\theta}) + \log P(\boldsymbol{\theta})]
$$

This equation is key.
* The first term, $\log P(\text{Data}|\boldsymbol{\theta})$, is the **log-likelihood**, which corresponds to our original loss function (e.g., MSE).
* The second term, $\log P(\boldsymbol{\theta})$, is the **log-prior**. This term is where regularization comes from!

**Ridge and the Gaussian Prior:**
If we assume a **Gaussian (Normal) prior** for our weights, centered at zero ($ \theta_j \sim \mathcal{N}(0, \sigma^2) $), it expresses a belief that the weights are likely to be small and close to zero. The log of this Gaussian prior is:
$$
\log P(\boldsymbol{\theta}) \propto -\frac{1}{2\sigma^2}\sum_{j=1}^{p} \theta_j^2
$$
This is precisely the L2 penalty term. Therefore, **Ridge regression is equivalent to MAP estimation with a Gaussian prior on the parameters.**

**Lasso and the Laplacian Prior:**
If we assume a **Laplace prior** for our weights, which has a sharper peak at zero compared to a Gaussian, it expresses a stronger belief that weights might be exactly zero. The log of the Laplace prior is:
$$
\log P(\boldsymbol{\theta}) \propto -\frac{1}{b}\sum_{j=1}^{p} |\theta_j|
$$
This is the L1 penalty term. Therefore, **Lasso regression is equivalent to MAP estimation with a Laplacian prior on the parameters.** This explains mathematically why Lasso performs feature selection.

This Bayesian view elevates regularization from a simple "hack" to prevent overfitting to a principled method of incorporating prior beliefs into our models.