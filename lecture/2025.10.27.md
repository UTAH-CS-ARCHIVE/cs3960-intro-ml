# [Lecture Note] Linear Regression

> Written by Hongseo Jang

## 1. The Linear Regression Model

Linear Regression is a fundamental supervised learning algorithm used to predict a continuous target variable based on one or more input features. The core idea is to find the best-fitting linear relationship between the features and the target.

### Hypothesis Function

The model's prediction is represented by a **hypothesis function**, denoted $h_{\boldsymbol{\theta}}(x)$.

* **Simple Linear Regression:** This model involves a single input feature ($x$) and has the familiar form of a straight line:
    $$
    h_{\theta}(x) = \theta_0 + \theta_1 x
    $$
    * $\theta_0$ is the y-intercept (or bias term).
    * $\theta_1$ is the slope (or the weight associated with the feature $x$).

* **Multiple Linear Regression:** This model uses multiple input features ($x_1, x_2, \dots, x_n$):
    $$
    h_{\boldsymbol{\theta}}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n
    $$
    This represents a hyperplane in a higher-dimensional space.

* **Vectorized Form:** For efficiency and mathematical elegance, we use a vectorized representation. By setting $x_0 = 1$ and combining our parameters and features into vectors $\boldsymbol{\theta}$ and $\mathbf{x}$, we can write the hypothesis as a dot product:
    $$
    h_{\boldsymbol{\theta}}(\mathbf{x}) = \boldsymbol{\theta}^T \mathbf{x}
    $$
    where $\boldsymbol{\theta} = [\theta_0, \theta_1, \dots, \theta_n]^T$ and $\mathbf{x} = [x_0, x_1, \dots, x_n]^T$.

<br>

## 2. The Cost Function: Mean Squared Error (MSE)

To find the "best-fitting" line, we need a way to measure how well a given line (defined by a set of parameters $\boldsymbol{\theta}$) fits our data. This is the role of the **cost function**, $J(\boldsymbol{\theta})$. The goal of training is to find the values of $\boldsymbol{\theta}$ that minimize this function.

For linear regression, the standard cost function is the **Mean Squared Error (MSE)**.

### Intuition

1.  For each training example $(x^{(i)}, y^{(i)})$, the error is the vertical distance between the actual value $y^{(i)}$ and the predicted value $h_{\boldsymbol{\theta}}(x^{(i)})$.
2.  We square this error, $(h_{\boldsymbol{\theta}}(x^{(i)}) - y^{(i)})^2$, to make all errors positive and to penalize larger errors more significantly.
3.  We sum these squared errors across all $m$ training examples and then take the average to get a single measure of the model's overall error.

### Formula

The MSE cost function is defined as:

$$
J(\boldsymbol{\theta}) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\boldsymbol{\theta}}(x^{(i)}) - y^{(i)})^2
$$

* $m$ is the number of training examples.
* The $\frac{1}{m}$ term makes it an average error.
* The $\frac{1}{2}$ is included for mathematical convenience; it simplifies the derivative when we use Gradient Descent, as it cancels out the exponent '2' from the squared term.

This cost function is **convex**, meaning it has a single global minimum. This guarantees that we can find the one optimal set of parameters $\boldsymbol{\theta}$ for our model.

<br>

## 3. The Normal Equation: An Analytical Solution

There are two primary methods to minimize the cost function $J(\boldsymbol{\theta})$:
1.  **Iterative methods** like Gradient Descent.
2.  **An analytical method** that solves for $\boldsymbol{\theta}$ directly, called the **Normal Equation**.

The Normal Equation provides a closed-form solution for the optimal parameters without the need for iterations or choosing a learning rate.

### Derivation Sketch

To find the minimum of the cost function $J(\boldsymbol{\theta})$, we can use calculus. We take the partial derivative of $J(\boldsymbol{\theta})$ with respect to each parameter $\theta_j$ and set all of them to zero. This gives us a system of linear equations. Solving this system for $\boldsymbol{\theta}$ yields the Normal Equation.

In matrix form, we set the gradient of the cost function to zero: $\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = 0$. After some matrix calculus and algebraic manipulation, we arrive at the solution.

### Formula

The Normal Equation is:

$$
\boldsymbol{\theta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$

* $\mathbf{X}$ is the **design matrix** of size $m \times (n+1)$. Each row is a training example, and a column of ones ($x_0=1$) is added to accommodate the intercept term $\theta_0$.
* $\mathbf{y}$ is the $m \times 1$ vector of target values.
* $(\mathbf{X}^T\mathbf{X})^{-1}$ is the inverse of the matrix $\mathbf{X}^T\mathbf{X}$.

### Comparison: Normal Equation vs. Gradient Descent

| Feature               | Normal Equation                               | Gradient Descent                              |
| --------------------- | --------------------------------------------- | --------------------------------------------- |
| **Approach** | Analytical, direct solution                   | Iterative, step-by-step approach              |
| **Learning Rate $\eta$** | Not required                                  | Must be chosen carefully                      |
| **Feature Scaling** | Not necessary                                 | Recommended for faster convergence            |
| **Computational Cost**| High. Inverting a matrix is $O(n^3)$.        | Lower per iteration. Scales well with $n$.    |
| **Best Use Case** | Small number of features (e.g., $n < 10,000$) | Large number of features                      |

**Important Note:** The matrix $(\mathbf{X}^T\mathbf{X})$ must be invertible. If it is not (i.e., it is singular), it usually means you have redundant features (e.g., `size_in_feet` and `size_in_meters`) or too many features ($m \le n$). In such cases, regularization techniques may be needed.