# [Lecture Note] Probability & Statistics II

> Written by Hongseo Jang

## 1. Random Variables

A **random variable** is a variable whose value is a numerical outcome of a random phenomenon. It maps outcomes from a sample space to a measurable space, typically the set of real numbers. We usually denote random variables with capital letters like $X$ or $Y$.

There are two main types of random variables:

* **Discrete Random Variable:** A variable that can only take on a finite or countably infinite number of distinct values.
    * *Examples:* The number of heads in three coin flips ({0, 1, 2, 3}), the result of a die roll ({1, 2, 3, 4, 5, 6}), or the number of emails you receive in an hour.
* **Continuous Random Variable:** A variable that can take on any value within a given range or interval.
    * *Examples:* The height of a person, the temperature of a room, or the time it takes to complete a task.

<br>

## 2. Probability Distributions

A **probability distribution** is a mathematical function that describes the probability of different possible values of a random variable.

* **For Discrete Variables (Probability Mass Function - PMF):**
    The PMF, denoted as $P(X=x)$, gives the probability that the discrete random variable $X$ is exactly equal to some value $x$. A valid PMF must satisfy two conditions:
    1.  $0 \le P(X=x) \le 1$ for all possible values of $x$.
    2.  $\sum_{x} P(X=x) = 1$ (The sum of probabilities over all possible outcomes is 1).

* **For Continuous Variables (Probability Density Function - PDF):**
    The PDF, denoted as $f(x)$, describes the relative likelihood for a continuous random variable to take on a given value. The probability is not the value of the function itself, but the *area under the curve* over an interval.
    1.  $f(x) \ge 0$ for all $x$.
    2.  The probability of $X$ falling within an interval $[a, b]$ is $P(a \le X \le b) = \int_{a}^{b} f(x)dx$.
    3.  $\int_{-\infty}^{\infty} f(x)dx = 1$ (The total area under the curve is 1).

<br>

## 3. Expected Value, Variance, and Covariance

* **Expected Value ($E[X]$ or $\mu$):**
    The expected value is the long-term average value of a random variable. It's a weighted average of all possible values, where the weights are their probabilities.
    * Discrete:
        $$
        E[X] = \sum_{x} x P(X=x)
        $$
    * Continuous:
        $$
        E[X] = \int_{-\infty}^{\infty} x f(x)dx
        $$

* **Variance ($Var(X)$ or $\sigma^2$):**
    Variance measures the spread or dispersion of a distribution. It is the expected value of the squared deviation from the mean.
    $$
    Var(X) = E[(X - \mu)^2]
    $$
    A more convenient formula for calculation is:
    $$
    Var(X) = E[X^2] - (E[X])^2
    $$
    The **standard deviation** ($\sigma$) is simply the square root of the variance, $\sigma = \sqrt{Var(X)}$.

* **Covariance ($Cov(X, Y)$):**
    Covariance measures the joint variability of two random variables. It indicates the direction of the linear relationship between them.
    $$
    Cov(X, Y) = E[(X - \mu_X)(Y - \mu_Y)]
    $$
    * If $Cov(X, Y) > 0$, the variables tend to move in the same direction.
    * If $Cov(X, Y) < 0$, they tend to move in opposite directions.
    * If $Cov(X, Y) = 0$, there is no linear relationship (though there could still be a non-linear one).

<br>

## 4. Key Probability Distributions in Machine Learning

Specific probability distributions are often used as assumptions in ML models, simplifying the model and allowing us to reason about the data.

* **Bernoulli Distribution:** Represents the outcome of a single trial with two possible results (e.g., success/failure, 1/0, heads/tails).
    * It is governed by a single parameter $p$, the probability of success.
    * PMF: $P(X=k) = p^k (1-p)^{1-k}$ for $k \in \{0, 1\}$.
    * **ML Connection:** This is the building block for binary classification problems. The output of a logistic regression model, for instance, can be interpreted as the parameter $p$ of a Bernoulli distribution for the class label.

* **Binomial Distribution:** Describes the number of successes in a fixed number, $n$, of independent Bernoulli trials, each with the same probability of success, $p$.
    * PMF: $P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$.
    * **ML Connection:** Useful for modeling binary outcome data that is repeated, like "out of 100 website visitors, how many will click the ad?".

* **Normal (Gaussian) Distribution:** A continuous distribution characterized by its bell shape. It is ubiquitous in statistics due to the Central Limit Theorem.
    * Parameters: mean ($\mu$) and variance ($\sigma^2$).
    * PDF:
        $$
        f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{ - \frac{(x-\mu)^2}{2\sigma^2} }
        $$
    * **ML Connection:** Many algorithms assume features or errors are normally distributed. For example, Linear Regression assumes the error terms ($y - \hat{y}$) follow a normal distribution. Gaussian Mixture Models (GMM) use a sum of Gaussians to perform clustering.

<br>

## 5. Maximum Likelihood Estimation (MLE)

MLE is a fundamental method for estimating the parameters of a statistical model. The core idea is to find the parameter values that maximize the probability (likelihood) of observing the actual data that was collected.

### Mathematical Principle

1.  **Likelihood Function:** Assume we have data $D = \{x_1, x_2, ..., x_n\}$ drawn from a distribution with unknown parameters $\theta$. The likelihood function, $L(\theta | D)$, is the probability of observing this data given the parameters. If the data points are independent and identically distributed (i.i.d.), the likelihood is the product of the individual probabilities:
    $$
    L(\theta | D) = P(x_1, ..., x_n | \theta) = \prod_{i=1}^{n} P(x_i | \theta)
    $$

2.  **Log-Likelihood:** Products are difficult to differentiate. Since the logarithm is a monotonically increasing function, maximizing the likelihood is equivalent to maximizing the log-likelihood. This turns the product into a sum, which is much easier to work with.
    $$
    \ell(\theta | D) = \log L(\theta | D) = \sum_{i=1}^{n} \log P(x_i | \theta)
    $$

3.  **Maximization:** We find the parameters $\hat{\theta}_{MLE}$ that maximize this log-likelihood function. This is typically done using calculus: by taking the derivative of $\ell$ with respect to $\theta$, setting it to zero, and solving for $\theta$.
    $$
    \hat{\theta}_{MLE} = \arg\max_{\theta} \ell(\theta | D)
    $$