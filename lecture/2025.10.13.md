# [Lecture Note] Calculus for Machine Learning

> Written by Hongseo Jang

## 1. The Derivative: A Measure of Change

In its simplest form, for a function of a single variable $f(x)$, the **derivative**, denoted as $f'(x)$ or $\frac{df}{dx}$, measures the instantaneous rate of change of the function with respect to its variable.

* **Formal Definition:** The derivative is defined by the limit:
    $$
    f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
    $$
* **Geometric Interpretation:** Geometrically, the derivative at a point $x$ gives the **slope of the tangent line** to the graph of the function at that point.
* **Relevance to ML:** In machine learning, we are constantly working with a cost function, $J(\theta)$, which measures how poorly our model is performing. The derivative $\frac{dJ}{d\theta}$ tells us how a small change in a model parameter $\theta$ will affect the cost. This is the key to knowing how to adjust $\theta$ to improve the model.

<br>

## 2. Partial Derivatives

Most functions in machine learning involve multiple variables (e.g., many different weights and biases in a neural network). To understand how the function changes in this multi-variable context, we use **partial derivatives**.

The partial derivative of a function $f(x_1, x_2, \dots, x_n)$ with respect to one of its variables, say $x_i$, is denoted as $\frac{\partial f}{\partial x_i}$.

To compute it, we differentiate $f$ with respect to $x_i$ while treating all other variables ($x_j$ where $j \neq i$) as if they were constants.

* **Example:** For a function $f(x, y) = 3x^2y + y^3$:
    * The partial derivative with respect to $x$ is:
        $\frac{\partial f}{\partial x} = 6xy$ (we treated $y$ as a constant).
    * The partial derivative with respect to $y$ is:
        $\frac{\partial f}{\partial y} = 3x^2 + 3y^2$ (we treated $x^2$ as a constant coefficient).

<br>

## 3. The Gradient

The **gradient** is a central concept in multi-variable calculus and machine learning optimization. For a multivariate function $f(\mathbf{x})$, where $\mathbf{x} = [x_1, x_2, \dots, x_n]^T$ is a vector of variables, the gradient is the vector of all its partial derivatives.

It is denoted by $\nabla f(\mathbf{x})$:

$$
\nabla f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix} = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right]^T
$$

* **Geometric Meaning:** The gradient is not just a collection of derivatives; it has a profound geometric meaning. At any point $\mathbf{x}$, the gradient vector $\nabla f(\mathbf{x})$ points in the direction of the **steepest ascent** of the function $f$. The magnitude of the gradient, $\|\nabla f(\mathbf{x})\|$, indicates the slope in that direction.

* **Role in Gradient Descent:** In machine learning, we want to *minimize* a cost function, not maximize it. Since the gradient $\nabla f$ points in the direction of the fastest increase, the negative gradient $-\nabla f$ must point in the direction of the **steepest descent**. This is the core principle behind the **gradient descent** algorithm, where we iteratively update our model's parameters by taking steps in the direction opposite to the gradient, thereby moving towards a minimum of the cost function.

<br>

## 4. The Chain Rule and Backpropagation

### The Chain Rule

The chain rule is a formula to compute the derivative of a composite function. If we have nested functions, say $L = f(z)$ and $z = g(W)$, the chain rule allows us to find the derivative of the outer function with respect to the inner variable:
$$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial W}
$$
In a neural network, the final loss is a deeply nested composite function of all the weights and biases in the network. For example, the loss $L$ depends on the output of the last layer, which depends on the output of the layer before it, and so on, all the way back to the initial weights.

### Backpropagation

**Backpropagation** is not a new type of calculus; it is simply an algorithm that efficiently applies the **chain rule** recursively to compute the gradients for all the parameters in a neural network.

The process works as follows:
1.  **Forward Pass:** An input is fed into the network, and its activations are computed layer by layer until we get the final output. This output is used to calculate the overall loss (e.g., the difference between the predicted and actual values).
2.  **Backward Pass:** The algorithm starts at the end, with the loss function. It first computes the gradient of the loss with respect to the output of the final layer. Then, using the chain rule, it works its way backward through the network, layer by layer. At each layer, it computes the gradient of the loss with respect to that layer's parameters (weights and biases) and its inputs.