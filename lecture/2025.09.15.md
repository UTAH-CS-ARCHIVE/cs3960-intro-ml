# [Lecture Note] Probability & Statistics I

> Written by Hongseo Jang

## 1. Fundamental Concepts of Probability

Probability is a measure of the likelihood that an event will occur. It's quantified as a number between 0 and 1, where 0 indicates impossibility and 1 indicates certainty.

* **Sample Space (S):** The set of all possible outcomes of a random experiment. For example, when rolling a single six-sided die, the sample space is $S = \{1, 2, 3, 4, 5, 6\}$.
* **Event (E):** A subset of the sample space. It's a specific outcome or a set of outcomes. For example, the event of rolling an even number is $E = \{2, 4, 6\}$.
* **Probability of an Event:** For a sample space with equally likely outcomes, the probability of an event E is the ratio of the number of favorable outcomes to the total number of outcomes.
    $$
    P(E) = \frac{\text{Number of outcomes in E}}{\text{Total number of outcomes in S}}
    $$

<br>

## 2. Conditional Probability

Conditional probability is the probability of an event occurring, given that another event has already occurred. The conditional probability of event A occurring given that event B has occurred is denoted by $P(A|B)$.

The formula for conditional probability is derived from the multiplication rule of probability:
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
where:
* $P(A \cap B)$ is the probability that both events A and B occur (the joint probability).
* $P(B)$ is the probability of event B occurring. It's important that $P(B) > 0$.

This formula essentially redefines the sample space. When we know that B has occurred, our new sample space is B, and we are interested in the part of A that also lies within B.

<br>

## 3. Independent Events

Two events, A and B, are considered independent if the occurrence of one does not affect the probability of the occurrence of the other.

Mathematically, A and B are independent if and only if:
$$P(A \cap B) = P(A)P(B)$$

From the conditional probability formula, if A and B are independent:
$P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A)P(B)}{P(B)} = P(A)$

This makes intuitive sense: if the events are independent, knowing that B has occurred gives us no new information about the probability of A occurring.

<br>

## 4. Bayes' Theorem

Bayes' Theorem is a fundamental theorem in probability theory that describes the probability of an event based on prior knowledge of conditions that might be related to the event. It's a way to update our beliefs in light of new evidence.

### Derivation & Formula

The theorem can be derived directly from the definition of conditional probability:
1.  $P(H|E) = \frac{P(H \cap E)}{P(E)}$
2.  $P(E|H) = \frac{P(E \cap H)}{P(H)}$

Since the joint probability is symmetric ($P(H \cap E) = P(E \cap H)$), we can rearrange the second equation to $P(E \cap H) = P(E|H)P(H)$.

Substituting this back into the first equation gives us the famous Bayes' Theorem:

$$P(H|E) = \frac{P(E|H)P(H)}{P(E)}$$

### Components of the Theorem

Let's break down the terms in the context of a hypothesis (H) and evidence (E):

* **$P(H|E)$ - Posterior Probability:** This is the probability of the hypothesis $H$ being true, *after* observing the evidence $E$. It represents our updated belief.

* **$P(E|H)$ - Likelihood:** This is the probability of observing the evidence $E$, *given* that the hypothesis $H$ is true. It quantifies how well the hypothesis explains the evidence.

* **$P(H)$ - Prior Probability:** This is the initial probability of the hypothesis $H$ being true, *before* considering the evidence. It represents our prior belief or knowledge.

* **$P(E)$ - Evidence (or Marginal Likelihood):** This is the total probability of observing the evidence $E$. It's a normalization constant that ensures the posterior probabilities sum to 1. It can be calculated using the law of total probability:
    $$
    P(E) = \sum_{i} P(E|H_i)P(H_i)
    $$
    For a simple case with a hypothesis $H$ and its complement $\neg H$, this would be $P(E) = P(E|H)P(H) + P(E|\neg H)P(\neg H)$.

<br>

## 5. Role of Bayes' Theorem in Machine Learning

Bayes' Theorem is not just a theoretical concept; it's the backbone of several powerful machine learning algorithms and a paradigm for reasoning under uncertainty.

### Naive Bayes Classifier

The Naive Bayes classifier is a simple but surprisingly effective probabilistic classifier based on applying Bayes' theorem. It's primarily used for classification tasks.

Let's say we have a set of features $X = (x_1, x_2, ..., x_n)$ and we want to predict a class $C_k$. We want to find the class that has the highest posterior probability, given the features:
$$\text{prediction} = \arg\max_{k} P(C_k|x_1, x_2, ..., x_n)$$

Using Bayes' Theorem, we can rewrite this as:
$$P(C_k|X) = \frac{P(X|C_k)P(C_k)}{P(X)}$$

Since $P(X)$ is the same for all classes, it's a constant we can ignore for the purpose of finding the maximum. So we are trying to maximize:
$$\arg\max_{k} P(X|C_k)P(C_k)$$

Here comes the "naive" assumption. The algorithm assumes that all features are **conditionally independent** given the class. This is a strong assumption and often not true in reality, but it simplifies the computation immensely. With this assumption, the likelihood $P(X|C_k)$ becomes:
$$P(X|C_k) = P(x_1, x_2, ..., x_n | C_k) = \prod_{i=1}^{n} P(x_i | C_k)$$

So, the final classification rule is to choose the class $C_k$ that maximizes the product of the prior and the individual likelihoods:
$$\text{prediction} = \arg\max_{k} P(C_k) \prod_{i=1}^{n} P(x_i | C_k)$$
This is commonly used in text classification, such as spam filtering, where the features might be the presence or absence of certain words.

### Bayesian Inference

Bayesian inference is a broader statistical framework that uses Bayes' theorem to update beliefs about parameters. It contrasts with the frequentist approach.

* **Frequentist View:** Model parameters are fixed, unknown constants. Probability describes the frequency of data outcomes in repeated experiments.
* **Bayesian View:** Model parameters are random variables about which we can have uncertainty. Probability describes our degree of belief in a proposition.

The process of Bayesian inference is:
1.  **Define a Prior Distribution:** We start by defining a prior probability distribution $P(\theta)$ over the model parameters $\theta$. This distribution represents our beliefs about $\theta$ before seeing any data.
2.  **Define a Likelihood:** We define the likelihood function $P(\text{Data}|\theta)$, which is the probability of observing the data given specific values of the parameters.
3.  **Compute the Posterior Distribution:** After observing the data, we use Bayes' theorem to compute the posterior distribution of the parameters:
    $$
    P(\theta|\text{Data}) = \frac{P(\text{Data}|\theta)P(\theta)}{P(\text{Data})}
    $$