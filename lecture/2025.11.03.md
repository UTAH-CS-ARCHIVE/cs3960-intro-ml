# [Lecture Note] Logistic Regression

> Written by Hongseo Jang

## 1. Introduction to Classification

So far, we have focused on regression problems, where the goal is to predict a continuous output value. Now, we shift to **classification problems**, where the goal is to predict a discrete class label.

* **Examples:**
    * Email: Spam or Not Spam? (Binary Classification)
    * Tumor: Malignant or Benign? (Binary Classification)
    * Handwritten Digit: 0, 1, 2, ..., or 9? (Multi-class Classification)

For this lecture, we will focus on **binary classification**, where the output label $y$ can only take on two values, typically 0 (the negative class) and 1 (the positive class).

### Why Not Use Linear Regression?

If we try to use linear regression for a classification task, we run into problems. The output of a linear regression model can be any real number, but we want a probability that is bounded between 0 and 1. Also, linear regression is sensitive to outliers, which can drastically shift the decision threshold and lead to poor classification performance. We need a model specifically designed for this task.

<br>

## 2. The Sigmoid (Logistic) Function

Logistic Regression is a classification algorithm that models the probability of a discrete outcome. It starts with the same linear equation as linear regression, but with a crucial difference.

1.  First, we calculate a linear combination of the inputs and parameters:
    $$
    z = \boldsymbol{\theta}^T \mathbf{x}
    $$
2.  Then, we pass this result $z$ through a **Sigmoid Function** (also called the Logistic Function), denoted $g(z)$. This function squashes the output to be in the range (0, 1).

### The Sigmoid Function Formula

$$
g(z) = \frac{1}{1 + e^{-z}}
$$



### Properties and Role

* **Output Range:** The sigmoid function always outputs a value between 0 and 1, regardless of the input $z$.
    * As $z \to \infty$, $g(z) \to 1$.
    * As $z \to -\infty$, $g(z) \to 0$.
* **Probabilistic Interpretation:** This allows us to interpret the output of our hypothesis function, $h_{\boldsymbol{\theta}}(\mathbf{x}) = g(\boldsymbol{\theta}^T \mathbf{x})$, as a probability. Specifically, it represents the estimated probability that the output is 1, given the input $\mathbf{x}$ and parameters $\boldsymbol{\theta}$.
    $$
    h_{\boldsymbol{\theta}}(\mathbf{x}) = P(y=1 | \mathbf{x}; \boldsymbol{\theta})
    $$
* **Decision Boundary:** To make a discrete prediction (0 or 1), we set a threshold, typically 0.5.
    * Predict $y=1$ if $h_{\boldsymbol{\theta}}(\mathbf{x}) \ge 0.5$, which happens when $z = \boldsymbol{\theta}^T \mathbf{x} \ge 0$.
    * Predict $y=0$ if $h_{\boldsymbol{\theta}}(\mathbf{x}) < 0.5$, which happens when $z = \boldsymbol{\theta}^T \mathbf{x} < 0$.
    The line or surface where the decision changes (i.e., where $\boldsymbol{\theta}^T \mathbf{x} = 0$) is called the **decision boundary**.

<br>

## 3. Binary Cross-Entropy Loss Function

For linear regression, we used Mean Squared Error (MSE) as our cost function. If we apply MSE to the logistic regression hypothesis (which includes the non-linear sigmoid function), the resulting cost function becomes **non-convex**. This is problematic because gradient descent is not guaranteed to find the global minimum.

We need a new cost function that is convex for this problem and is well-suited for measuring the error in probability estimates. This function is called **Binary Cross-Entropy** (or Log Loss), and it can be derived from the principle of **Maximum Likelihood Estimation (MLE)**.

### Derivation from MLE

The goal of MLE is to find the model parameters ($\boldsymbol{\theta}$) that maximize the likelihood of observing the given training data.

1.  **Probability of a Single Outcome:** We can express the probability of observing a single label $y^{(i)}$ given input $x^{(i)}$ in a single, clever equation:
    $$
    P(y^{(i)} | x^{(i)}; \boldsymbol{\theta}) = (h_{\boldsymbol{\theta}}(x^{(i)}))^{y^{(i)}} (1 - h_{\boldsymbol{\theta}}(x^{(i)}))^{1-y^{(i)}}
    $$
    (Notice that if $y^{(i)}=1$, this simplifies to $h_{\boldsymbol{\theta}}(x^{(i)})$. If $y^{(i)}=0$, it simplifies to $1 - h_{\boldsymbol{\theta}}(x^{(i)})$.)

2.  **Likelihood of the Dataset:** Assuming our training examples are independent, the total likelihood $L(\boldsymbol{\theta})$ is the product of the probabilities of each individual example:
    $$
    L(\boldsymbol{\theta}) = \prod_{i=1}^{m} P(y^{(i)} | x^{(i)}; \boldsymbol{\theta})
    $$

3.  **Log-Likelihood:** To make the math easier (turning products into sums), we work with the log-likelihood $\ell(\boldsymbol{\theta})$:
    $$
    \ell(\boldsymbol{\theta}) = \log L(\boldsymbol{\theta}) = \sum_{i=1}^{m} [y^{(i)}\log(h_{\boldsymbol{\theta}}(x^{(i)})) + (1-y^{(i)})\log(1-h_{\boldsymbol{\theta}}(x^{(i)}))]
    $$

4.  **Defining the Cost Function:** Our goal is to maximize the log-likelihood. In machine learning, we typically frame problems as minimizing a cost function. Maximizing a function is equivalent to minimizing its negative. Therefore, we define our cost function $J(\boldsymbol{\theta})$ as the negative average log-likelihood:

    $$
    J(\boldsymbol{\theta}) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)}\log(h_{\boldsymbol{\theta}}(x^{(i)})) + (1-y^{(i)})\log(1-h_{\boldsymbol{\theta}}(x^{(i)}))]
    $$

### Intuition behind the Loss

This function penalizes confident but incorrect predictions heavily.
* **Case 1: True label is 1 ($y=1$)**
    * The cost is $-\log(h_{\boldsymbol{\theta}}(x))$.
    * If the model correctly predicts a high probability (e.g., $h_{\boldsymbol{\theta}}(x) = 0.99$), the cost $-\log(0.99)$ is very close to 0.
    * If the model incorrectly predicts a low probability (e.g., $h_{\boldsymbol{\theta}}(x) = 0.01$), the cost $-\log(0.01)$ is very large.
* **Case 2: True label is 0 ($y=0$)**
    * The cost is $-\log(1 - h_{\boldsymbol{\theta}}(x))$.
    * If the model correctly predicts a low probability (e.g., $h_{\boldsymbol{\theta}}(x) = 0.01$), the cost $-\log(0.99)$ is very close to 0.
    * If the model incorrectly predicts a high probability (e.g., $h_{\boldsymbol{\theta}}(x) = 0.99$), the cost $-\log(0.01)$ is very large.