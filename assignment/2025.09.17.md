### **Assignment 8: Generative vs. Discriminative Models**

**Question 1: Foundational Concepts**
In probabilistic terms, what is the fundamental difference between a generative and a discriminative classification model? State the primary probability distribution that each type of model aims to learn.

<br>

**Question 2: Derivation of the Generative Classifier**
Starting with Bayes' theorem, derive the decision rule for a generative classifier. Explain why the denominator, $P(\mathbf{x})$, can be disregarded during the classification process. The final decision rule should be expressed using the `argmax` function.

<br>

**Question 3: The Naive Bayes Assumption**
The Naive Bayes classifier is described as "naive." Express this core assumption mathematically. Let the input feature vector be $\mathbf{x} = (x_1, x_2, \dots, x_p)$. Write down the formula for the class-conditional probability $P(\mathbf{x}|y)$ that results from this assumption.

<br>

**Question 4: Model Comparison**
Contrast the mathematical formulation of Logistic Regression for binary classification with the decision rule of a Naive Bayes classifier. Specifically, write down the formula for $P(y=1|\mathbf{x})$ in Logistic Regression and explain how its direct modeling approach differs from the component-based approach of Naive Bayes.

<br>
<br>

### **Solutions Report: Assignment 1**

**Report Summary:** This document provides the official solutions for Assignment 1. The solutions are derived directly from the principles discussed in the lecture notes on generative and discriminative models.

<br>

**Solution 1: Foundational Concepts**
The fundamental difference lies in the scope of the probability distribution they model.

* A **generative model** learns the **joint probability distribution** $P(\mathbf{x}, y)$. It models how the features $\mathbf{x}$ and the class label $y$ are generated together.
* A **discriminative model** directly learns the **conditional probability distribution** $P(y|\mathbf{x})$. It focuses solely on predicting the label $y$ given the features $\mathbf{x}$, without learning about the distribution of the features themselves.

<br>

**Solution 2: Derivation of the Generative Classifier**
The goal of classification is to find the class label $y$ that is most probable given the input features $\mathbf{x}$. This is the posterior probability $P(y|\mathbf{x})$.

1.  We start with Bayes' theorem:

    $$
    P(y|\mathbf{x}) = \frac{P(\mathbf{x}|y)P(y)}{P(\mathbf{x})}
    $$

    Here, $P(\mathbf{x}|y)$ is the likelihood, $P(y)$ is the prior, and $P(\mathbf{x})$ is the evidence.

2.  To find the most probable class, we seek the label $\hat{y}$ that maximizes this posterior probability:

    $$
    \hat{y} = \arg\max_{y} P(y|\mathbf{x}) = \arg\max_{y} \frac{P(\mathbf{x}|y)P(y)}{P(\mathbf{x})}
    $$

3.  The term $P(\mathbf{x})$ in the denominator represents the probability of observing the feature vector $\mathbf{x}$, which is constant for all classes $y$. Since it does not depend on $y$, it does not affect the maximization. Therefore, we can disregard it.

4.  This simplifies the decision rule to:

    $$
    \hat{y} = \arg\max_{y} P(\mathbf{x}|y)P(y)
    $$

    This is the final decision rule for a generative classifier, which relies on the likelihood and the prior.

<br>

**Solution 3: The Naive Bayes Assumption**
The "naive" assumption in the Naive Bayes classifier is the **conditional independence of features given the class**.

Mathematically, this means that for a feature vector $\mathbf{x} = (x_1, x_2, \dots, x_p)$, the probability of observing this entire vector given a class $y$ is assumed to be the product of the individual probabilities of observing each feature given that same class.

The resulting formula for the class-conditional probability (likelihood) is:

$$
P(\mathbf{x}|y) = P(x_1, x_2, \dots, x_p|y) = \prod_{i=1}^{p} P(x_i|y)
$$

<br>

**Solution 4: Model Comparison**
**Logistic Regression (Discriminative):**
For a binary classification problem ($y \in \{0, 1\}$), Logistic Regression directly models the posterior probability $P(y=1|\mathbf{x})$ using the logistic (sigmoid) function. It does not model $P(\mathbf{x}|y)$ or $P(y)$. The formulation is:

$$
P(y=1|\mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x} + b)}}
$$

This model directly learns the parameters $\mathbf{w}$ and $b$ that define the boundary separating the classes.

**Naive Bayes (Generative):**
In contrast, Naive Bayes does not model $P(y|\mathbf{x})$ directly. It models the components, the prior $P(y)$ and the likelihood $P(\mathbf{x}|y)$, and then combines them using Bayes' theorem to make a decision:

$$
\hat{y} = \arg\max_{y} P(y) \prod_{i=1}^{p} P(x_i|y)
$$

The core difference is that Logistic Regression learns a single decision boundary, whereas Naive Bayes learns a probabilistic model for each class and uses these models to define a decision boundary.