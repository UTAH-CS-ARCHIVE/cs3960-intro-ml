### üé≤ SDE (Stochastic Differential Equation)

A stochastic differential equation (SDE) is an equation that models **stochastic processes**, where both a **deterministic rate of change (drift)** and **random noise (stochastic component)** act on the system simultaneously.  

The basic form is:

$$d x_{t}=f(x_{t},t)d t$$

By adding a stochastic term, we get:

$$d x_{t}=f(x_{t},t)d t+g(x_{t},t)d W_{t}$$

- $d x_{t}=f(x_{t},t)d t$: **Drift** ‚Üí average (deterministic) trend  
- $g(x_{t},t)d W_{t}$: **Diffusion** ‚Üí random effect (stochastic part)  
- $W_t$: **Wiener process** ‚Üí Brownian motion  

In other words, a quantity $x_t$ evolves over time due to a deterministic force (drift $f$) while simultaneously being affected by unpredictable random shocks ($dW_t$).

The integral form is:

$$X(t)=x_{0}+\int_{0}^{t}f(X(s),s)\,d s+\int_{0}^{t}g(X(s),s)\,d W(s)$$

In many papers, this is written as:

$$X(t)=x_{0}+\int_{0}^{t}b(X(s))\,d s+\int_{0}^{t}\sigma(X(s))\,d W(s)$$

Here, $b$ and $\sigma$ represent the **drift** and **diffusion**, respectively.

---

### üìê RKHS (Reproducing Kernel Hilbert Space)

#### 1. What is RKHS?
- A Reproducing Kernel Hilbert Space (RKHS) is a function space defined by a kernel function $k(x,y)$.  
- Its key property is the **reproducing property**:

$$f(x)=\langle f,k(\cdot,x)\rangle_{\mathcal{H}}$$

- This means that the value of a function can be ‚Äúreproduced‚Äù through an **inner product**.  
- As a result, RKHS provides a space where functions can be measured with notions of length and angle, and point evaluation is well-defined.

---

#### 2. From Infinite-Dimensional to Finite-Dimensional Optimization
- Optimization problems over functions are typically **infinite-dimensional**, which makes them difficult to handle.  
- A crucial tool in RKHS is the **Representer Theorem**.

According to this theorem, the solution of a regularized learning problem using kernels can be expressed as a **finite linear combination of kernel functions**:

$$f^{*}(x)=\sum_{i=1}^{n}\alpha_{i}k(x,x_{i})$$

üëâ This means we don‚Äôt need to solve for an infinite-dimensional function directly.  
Instead, we only need to find the coefficients $\alpha_i$ for the given data points, turning the problem into a **finite-dimensional optimization problem**.

---

#### 3. Importance of the Reproducing Property and Representer Theorem
- Thanks to the **reproducing property**, function values can be computed as inner products, enabling data-driven optimization.  
- The **Representer Theorem** ensures that even though the problem is defined in an infinite-dimensional function space, practical computation reduces to a **finite-dimensional vector optimization problem**.  

Together, these properties form the mathematical foundation for methods such as **SVMs, kernel regression, and Gaussian processes** in machine learning and statistics.