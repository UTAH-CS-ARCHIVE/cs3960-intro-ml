## [Assignment] Simple and Multiple Regression(Calculation and Statistical Interpretation)

### Part A. Simple Linear Regression

We are given the following data:

| Study time (x) | Exam score (y) |
|:---:|:---:|
| 2 | 55 |
| 3 | 60 |
| 4 | 75 |
| 5 | 80 |
| 6 | 90 |

We model $y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ and estimate $(\hat\beta_0,\hat\beta_1)$ by least squares.

---

#### Step 1. Objective function (sum of squared errors)

$$
S(\beta_0,\beta_1)=\sum_{i=1}^n \big(y_i - \beta_0 - \beta_1 x_i\big)^2
$$

---

#### Step 2. Derive the normal equations

$$
\frac{\partial S}{\partial \beta_0}=-2\sum_{i=1}^n\big(y_i-\beta_0-\beta_1 x_i\big)=0
\quad\Rightarrow\quad
\sum y_i = n\beta_0 + \beta_1 \sum x_i
$$

$$
\frac{\partial S}{\partial \beta_1}=-2\sum_{i=1}^n x_i\big(y_i-\beta_0-\beta_1 x_i\big)=0
\quad\Rightarrow\quad
\sum x_i y_i = \beta_0 \sum x_i + \beta_1 \sum x_i^2
$$

Equivalently, the least squares estimators can be expressed as:

$$
\hat\beta_1=\frac{\sum (x_i-\bar x)(y_i-\bar y)}{\sum (x_i-\bar x)^2},\qquad
\hat\beta_0=\bar y-\hat\beta_1\,\bar x
$$

---

#### Step 3. Compute sample statistics

$$
\bar x=\frac{2+3+4+5+6}{5}=4,\qquad
\bar y=\frac{55+60+75+80+90}{5}=72
$$

| $x_i$ | $y_i$ | $x_i-\bar x$ | $y_i-\bar y$ | $(x_i-\bar x)(y_i-\bar y)$ | $(x_i-\bar x)^2$ |
|---:|---:|---:|---:|---:|---:|
| 2 | 55 | −2 | −17 | 34 | 4 |
| 3 | 60 | −1 | −12 | 12 | 1 |
| 4 | 75 | 0 | 3 | 0 | 0 |
| 5 | 80 | 1 | 8 | 8 | 1 |
| 6 | 90 | 2 | 18 | 36 | 4 |
|   |   |   |   | **90** | **10** |

Thus,

$$
\hat\beta_1=\frac{90}{10}=9,\qquad
\hat\beta_0=72-9\times 4 = 36
$$

**Fitted regression line:**

$$
\hat y = 36 + 9x
$$

---

#### Part A2. Interpretation of $\hat\beta_1$

The slope $\hat\beta_1=9$ indicates that, on average, **each additional hour of study time increases the exam score by about 9 points**. Practically, if two students differ by one hour of study time, the one who studied longer is expected to score about 9 points higher, all else equal.

---

### Part B. Multiple Regression

We are given the following data:

| Study time ($x_1$) | Class participation ($x_2$) | Exam score ($y$) |
|:---:|:---:|:---:|
| 2 | 70 | 55 |
| 3 | 85 | 60 |
| 4 | 90 | 75 |
| 5 | 80 | 80 |
| 6 | 95 | 90 |

We consider the model:

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \varepsilon_i
$$

---

#### Part B1. Question
Formulate the null hypotheses ($H_0$) for each predictor variable.  

#### Answer
For each variable, the null hypothesis states that it has no partial effect on the exam score:

**Study time ($x_1$):**

$$
H_{0,1}:\ \beta_1=0 \quad \text{vs.} \quad H_{A,1}:\ \beta_1\neq 0
$$

Interpretation: After controlling for class participation, study time does not affect exam score.

**Class participation ($x_2$):**

$$
H_{0,2}:\ \beta_2=0 \quad \text{vs.} \quad H_{A,2}:\ \beta_2\neq 0
$$

Interpretation: After controlling for study time, class participation does not affect exam score.

---

#### Part B2. Question
Explain why a large t-statistic for a predictor implies that the variable is statistically significant.  

#### Answer
For predictor $x_j$, the test statistic is:

$$
t_j=\frac{\hat\beta_j}{\operatorname{SE}(\hat\beta_j)}.
$$

- **Numerator (signal):** $\hat\beta_j$ is the estimated partial effect of $x_j$ on $y$. If $H_0$ is true ($\beta_j=0$), the true effect should be zero, so the estimate would fluctuate around zero.

- **Denominator (noise):** $\operatorname{SE}(\hat\beta_j)$ is the estimated standard error of $\hat\beta_j$. It reflects:
  1. The residual variability in $y$ not explained by the model, and  
  2. The geometry of the predictors (sample size, spread, and collinearity).  

Formally,

$$
\operatorname{SE}(\hat\beta_j)=\sqrt{\hat\sigma^2 (X^\top X)^{-1}_{jj}}, 
\qquad 
\hat\sigma^2=\frac{1}{n-p-1}\sum_{i=1}^n (y_i-\hat y_i)^2
$$

with $p=2$ predictors.

Thus, $t_j$ measures how many standard errors away from zero the estimate lies. Under $H_0$, $t_j$ follows a t-distribution with $\nu=n-p-1$ degrees of freedom.

- If $|t_j|$ is small, the estimate is close to zero relative to noise, consistent with $H_0$.
- If $|t_j|$ is large, the estimate is too extreme to be explained by random noise alone, leading to a small p-value and rejection of $H_0$.

**Conclusion:** A large t-statistic means the variable’s estimated effect is strong relative to uncertainty, which is why it indicates statistical significance.
