## Assignment 1: Linear Algebra Fundamentals

**Instructions:** Please answer the following questions. Show your work for all calculations.

**Problem 1: Vector Operations**
Given the following two vectors in $\mathbb{R}^3$:

$$
a = \begin{bmatrix} 2 \\ -1 \\ 3 \end{bmatrix}, \quad b = \begin{bmatrix} 4 \\ 2 \\ 0 \end{bmatrix}
$$

Calculate the following:
a) The dot product $a \cdot b$.
b) The L1 norm of vector $a$, $\|a\|_1$.
c) The L2 norm of vector $b$, $\|b\|_2$.
d) The cosine similarity between vectors $a$ and $b$. Based on the result, what can you infer about their relationship?

<br>

**Problem 2: Conceptual Understanding of Similarity Metrics**
In machine learning, we often use both **Euclidean distance** ($\|a - b\|_2$) and **cosine similarity** to measure the "distance" or "similarity" between two vectors. Explain the fundamental difference between what these two metrics measure. Provide a simple numerical example of two vectors that are "close" according to one metric but "far" according to the other, and explain why.

<br>

**Problem 3: Application of Norms in Machine Learning**
The lecture notes mention that the L1 norm is used in LASSO regression and tends to produce sparse solutions, while the L2 norm is used in Ridge regression. From a mathematical perspective, why does the L1 norm promote sparsity (i.e., drive parameters to exactly zero) more effectively than the L2 norm? (A geometric intuition is sufficient).

<br>
<br>

## Solution Report for Assignment 1

Here are the detailed solutions for the assignment.

### **Solution to Problem 1: Vector Operations**

Given vectors $a = [2, -1, 3]$ and $b = [4, 2, 0]$.

**a) Dot Product:**
The dot product is calculated as the sum of the element-wise products.

$$
a \cdot b = \sum_{i=1}^{3} a_i b_i = (2)(4) + (-1)(2) + (3)(0)
$$

$$
a \cdot b = 8 - 2 + 0 = 6
$$

**The dot product is 6.**

<br>

**b) L1 Norm of vector $a$:**
The L1 norm is the sum of the absolute values of the vector's components.

$$
\|a\|_1 = \sum_{i=1}^{3} |a_i| = |2| + |-1| + |3|
$$

$$
\|a\|_1 = 2 + 1 + 3 = 6
$$

**The L1 norm of $a$ is 6.**

<br>

**c) L2 Norm of vector $b$:**
The L2 norm is the square root of the sum of the squared components.

$$
\|b\|_2 = \sqrt{\sum_{i=1}^{3} b_i^2} = \sqrt{4^2 + 2^2 + 0^2}
$$

$$
\|b\|_2 = \sqrt{16 + 4 + 0} = \sqrt{20} \approx 4.472
$$

**The L2 norm of $b$ is $\sqrt{20}$.**

<br>

**d) Cosine Similarity:**
First, we need the L2 norm of vector $a$.

$$
\|a\|_2 = \sqrt{2^2 + (-1)^2 + 3^2} = \sqrt{4 + 1 + 9} = \sqrt{14}
$$

Now, we use the cosine similarity formula:

$$
\cos(\theta) = \frac{a \cdot b}{\|a\|_2 \|b\|_2} = \frac{6}{\sqrt{14} \cdot \sqrt{20}}
$$

$$
\cos(\theta) = \frac{6}{\sqrt{280}} \approx \frac{6}{16.733} \approx 0.3586
$$

**Inference:** The cosine similarity is approximately 0.3586. Since this value is positive but not close to 1, it indicates that the vectors are somewhat similar in direction, but are not pointing in the same or even a very similar direction. The angle between them is acute, specifically $\theta = \arccos(0.3586) \approx 69^\circ$. They are far from being orthogonal (value of 0) or identical in direction (value of 1).

<br>

### **Solution to Problem 2: Conceptual Understanding of Similarity Metrics**

**Fundamental Difference:**
* **Euclidean Distance** measures the straight-line distance between the points defined by the tips of the two vectors. It is sensitive to both the **magnitude** (length) and **direction** of the vectors. Two vectors are close if their endpoints are near each other in the vector space.
* **Cosine Similarity** measures the cosine of the angle between two vectors. It is concerned only with the **direction** or **orientation** of the vectors, not their magnitude. Two vectors are considered identical if they point in the exact same direction, regardless of their lengths.

**Example:**
Consider two vectors representing user ratings for a product:

$$
v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad v_2 = \begin{bmatrix} 100 \\ 100 \end{bmatrix}
$$

* **Euclidean Distance Calculation:**

    $$
    \|v_1 - v_2\|_2 = \sqrt{(1-100)^2 + (1-100)^2} = \sqrt{(-99)^2 + (-99)^2} = \sqrt{2 \cdot 9801} = \sqrt{19602} \approx 140
    $$

    By Euclidean distance, these vectors are very **far** apart.

* **Cosine Similarity Calculation:**

    $$
    \cos(\theta) = \frac{v_1 \cdot v_2}{\|v_1\|_2 \|v_2\|_2} = \frac{(1)(100) + (1)(100)}{\sqrt{1^2+1^2} \cdot \sqrt{100^2+100^2}} = \frac{200}{\sqrt{2} \cdot \sqrt{20000}} = \frac{200}{\sqrt{40000}} = \frac{200}{200} = 1
    $$
    
    By cosine similarity, these vectors are maximally **close** (identical).

**Explanation:** In this example, vector $v_2$ is simply a scaled-up version of $v_1$. They point in the exact same direction in a 2D space. Cosine similarity captures this perfectly with a score of 1. However, the Euclidean distance sees their endpoints as being very far apart and thus considers them dissimilar. This is crucial in applications like text analysis, where the length of a document (and thus the magnitude of its vector) might be less important than the relative frequencies of the words it contains (its direction).

<br>

### **Solution to Problem 3: Application of Norms in Machine Learning**

The reason L1 regularization promotes sparsity more than L2 regularization lies in the geometry of their respective "unit balls" (the set of all vectors with a norm of 1).

* **L1 Norm Geometry (Manhattan Norm):** The L1 unit ball in two dimensions ($|x_1| + |x_2| \leq 1$) has the shape of a diamond (a square rotated by 45 degrees). This diamond has sharp corners that lie on the axes.

* **L2 Norm Geometry (Euclidean Norm):** The L2 unit ball in two dimensions ($x_1^2 + x_2^2 \leq 1$) is a circle. It has no sharp corners and is smooth everywhere.

**Geometric Intuition:**
In regularization, we are trying to find model parameters (weights) that minimize a loss function, subject to the constraint that the norm of the weight vector is less than or equal to some constant $C$. This is equivalent to finding the first point where the expanding level curves (ellipses, typically) of the loss function touch the constraint region (the L1 or L2 ball).

* Because the **L1 ball has sharp corners on the axes**, it is highly probable that the elliptical level curves of the loss function will first make contact with the diamond shape at one of its corners. A point on a corner (e.g., [0, C]) means that one of the parameters is exactly zero. This is sparsity.

* In contrast, the **L2 ball is a circle**. Because it is perfectly round, the point of contact with the expanding ellipse can be anywhere on its circumference. It is statistically very unlikely for this point of contact to be exactly on an axis (e.g., [0, C]), where a parameter would be zero. Instead, the parameters will be small, but typically non-zero.

Therefore, the shape of the L1 constraint region naturally leads to solutions where some parameters are precisely zero, thus promoting model sparsity.