## Assignment 5: Eigenvectors, Eigendecomposition, and SVD

**Instructions:** Please provide detailed solutions for the following problems. Show all intermediate steps in your calculations and explain your conceptual reasoning clearly.

**Problem 1: Finding Eigenvalues and Eigenvectors**
For the following square matrix $\mathbf{A}$, find its eigenvalues and the corresponding eigenvectors.

$$
\mathbf{A} = \begin{pmatrix} 4 & -2 \\ 1 & 1 \end{pmatrix}
$$

a) First, find the eigenvalues by solving the characteristic equation $\det(\mathbf{A} - \lambda\mathbf{I}) = 0$.
b) For each eigenvalue found in part (a), find its corresponding eigenvector by solving the system $(\mathbf{A} - \lambda\mathbf{I})\mathbf{v} = 0$.


**Problem 2: Conceptual: Eigendecomposition vs. SVD**
Explain the fundamental differences between Eigendecomposition and Singular Value Decomposition (SVD). Your answer should address the following points:
a) The types of matrices that each decomposition can be applied to.
b) The properties of the matrices that result from each decomposition (i.e., compare matrix $\mathbf{Q}$ from eigendecomposition to matrices $\mathbf{U}$ and $\mathbf{V}$ from SVD).


**Problem 3: Application of SVD to Principal Component Analysis (PCA)**
Suppose you have a centered data matrix $\mathbf{X}$ and you have computed its Singular Value Decomposition, $\mathbf{X} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T$. The resulting matrices are:

$$
\mathbf{U} = \dots \quad \boldsymbol{\Sigma} = \begin{pmatrix} 20 & 0 \\ 0 & 5 \\ 0 & 0 \end{pmatrix} \quad \mathbf{V} = \begin{pmatrix} 0.6 & -0.8 \\ 0.8 & 0.6 \end{pmatrix}
$$

a) What are the principal components of the data?
b) The first principal component (PC1) captures the direction of maximum variance. If you have a data point (represented as a row vector) $\mathbf{x} = \begin{pmatrix} 10 & 5 \end{pmatrix}$, what is its new coordinate value after being projected onto the first principal component?


## Solution Report for Assignment 5

Below are the detailed solutions for the assignment problems.

### **Solution to Problem 1: Eigenvalues and Eigenvectors**

Given the matrix $\mathbf{A} = \begin{pmatrix} 4 & -2 \\ 1 & 1 \end{pmatrix}$.

**a) Finding Eigenvalues:**
We must solve the characteristic equation $\det(\mathbf{A} - \lambda\mathbf{I}) = 0$.

$$
\mathbf{A} - \lambda\mathbf{I} = \begin{pmatrix} 4 & -2 \\ 1 & 1 \end{pmatrix} - \lambda \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} 4-\lambda & -2 \\ 1 & 1-\lambda \end{pmatrix}
$$

Now, we compute the determinant:

$$
\det(\mathbf{A} - \lambda\mathbf{I}) = (4-\lambda)(1-\lambda) - (-2)(1) = 4 - 4\lambda - \lambda + \lambda^2 + 2 = \lambda^2 - 5\lambda + 6
$$

Set the determinant to zero and solve for $\lambda$:
$$
\lambda^2 - 5\lambda + 6 = 0
$$

$$
(\lambda - 2)(\lambda - 3) = 0
$$

**The eigenvalues are $\lambda_1 = 2$ and $\lambda_2 = 3$.**


**b) Finding Eigenvectors:**
For each eigenvalue, we solve $(\mathbf{A} - \lambda\mathbf{I})\mathbf{v} = 0$.

**Case 1: $\lambda_1 = 2$**

$$
(\mathbf{A} - 2\mathbf{I})\mathbf{v} = \begin{pmatrix} 4-2 & -2 \\ 1 & 1-2 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} 2 & -2 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
$$

This gives the equation $2v_1 - 2v_2 = 0$, which simplifies to $v_1 = v_2$. Any non-zero vector where the components are equal is a valid eigenvector. We can choose:
**The eigenvector for $\lambda_1 = 2$ is $\mathbf{v}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$.**

**Case 2: $\lambda_2 = 3$**

$$
(\mathbf{A} - 3\mathbf{I})\mathbf{v} = \begin{pmatrix} 4-3 & -2 \\ 1 & 1-3 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} 1 & -2 \\ 1 & -2 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
$$

This gives the equation $v_1 - 2v_2 = 0$, which means $v_1 = 2v_2$. We can choose $v_2 = 1$, which gives $v_1 = 2$.
**The eigenvector for $\lambda_2 = 3$ is $\mathbf{v}_2 = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$.**



### **Solution to Problem 2: Conceptual: Eigendecomposition vs. SVD**

Eigendecomposition and SVD are both methods to factor matrices, but they have key differences in their applicability and the nature of their components.

**a) Applicable Matrices:**
* **Eigendecomposition** ($\mathbf{A} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}$) is only defined for square matrices. Furthermore, it requires the matrix to be *diagonalizable*, meaning it must have a full set of linearly independent eigenvectors.
* **Singular Value Decomposition (SVD)** ($\mathbf{A} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T$) is a more general method. It can be applied to **any** rectangular $m \times n$ matrix, whether it is square or not.

**b) Properties of Resulting Matrices:**
* In **Eigendecomposition**, the matrix $\mathbf{Q}$ contains the eigenvectors of $\mathbf{A}$. While its columns are linearly independent, they are not necessarily orthogonal unless the original matrix $\mathbf{A}$ is symmetric. $\mathbf{Q}$ is simply an invertible matrix.
* In **SVD**, the matrices $\mathbf{U}$ and $\mathbf{V}$ are **orthogonal matrices**. This is a stricter condition, meaning their columns are orthonormal (mutually perpendicular and have a length of 1). This property of orthogonality is extremely useful and makes SVD numerically stable and geometrically intuitive (representing rotations/reflections).


### **Solution to Problem 3: Application of SVD to PCA**

We are given the SVD components for a centered data matrix $\mathbf{X}$, including $\mathbf{V} = \begin{pmatrix} 0.6 & -0.8 \\ 0.8 & 0.6 \end{pmatrix}$.

**a) Identifying the Principal Components:**
In PCA derived from SVD, the principal components are the right-singular vectors, which are the columns of the matrix $\mathbf{V}$.
* **First Principal Component (PC1):** The first column of $\mathbf{V}$, which is $\mathbf{v}_1 = \begin{pmatrix} 0.6 \\ 0.8 \end{pmatrix}$.
* **Second Principal Component (PC2):** The second column of $\mathbf{V}$, which is $\mathbf{v}_2 = \begin{pmatrix} -0.8 \\ 0.6 \end{pmatrix}$.

**b) Projecting a Data Point onto PC1:**
To find the new coordinate value of a data point $\mathbf{x}$ in the reduced dimensional space defined by PC1, we project $\mathbf{x}$ onto the first principal component vector, $\mathbf{v}_1$. This is done by computing the dot product of the data point vector and the principal component vector.

Given data point $\mathbf{x} = \begin{pmatrix} 10 & 5 \end{pmatrix}$ and PC1 vector $\mathbf{v}_1 = \begin{pmatrix} 0.6 \\ 0.8 \end{pmatrix}$.

$$
\text{Projected Value} = \mathbf{x} \cdot \mathbf{v}_1 = \begin{pmatrix} 10 & 5 \end{pmatrix} \begin{pmatrix} 0.6 \\ 0.8 \end{pmatrix}
$$

$$
= (10)(0.6) + (5)(0.8) = 6 + 4 = 10
$$

**The coordinate of the data point $\mathbf{x}$ in the 1-dimensional space defined by the first principal component is 10.**