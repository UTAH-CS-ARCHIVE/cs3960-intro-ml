# Part I. Feature Space and the Curse of Dimensionality

**1. Manifold hypothesis**  
The manifold hypothesis states that although data points $x \in \mathbb{R}^d$ live in a high-dimensional space, they actually lie near a low-dimensional manifold of dimension $m \ll d$.  

This hypothesis justifies dimension reduction methods such as PCA, t-SNE, or autoencoders because learning in the intrinsic manifold dimension $m$ can be more efficient and generalizable than in the ambient dimension $d$.

---

**2. Unit cube boundary volume**  
Consider the unit cube $[0,1]^d$. The proportion of volume within $\varepsilon$ of the boundary is:

$$
\text{Ratio} = 1 - (1 - 2\varepsilon)^d
$$

As $d \to \infty$,

$$
(1 - 2\varepsilon)^d \to 0 \quad \Rightarrow \quad \text{Ratio} \to 1
$$

Thus, almost all of the cube’s volume lies near the boundary.  

**Interpretation:** In high dimensions, points are pushed toward the boundary, so distances between points concentrate. This undermines nearest-neighbor search since all points become nearly equidistant.

---

**3. Gaussian concentration**  
Let $X \sim \mathcal{N}(0, I_d)$, so $X = (X_1,\dots,X_d)$ with $X_i \sim \mathcal{N}(0,1)$. Then

$$
\|X\|^2 = \sum_{i=1}^d X_i^2 \sim \chi^2(d)
$$

We know

$$
\mathbb{E}[\|X\|^2] = d, \quad \text{Var}(\|X\|^2) = 2d
$$

By Chebyshev’s inequality or concentration bounds,

$$
\frac{\|X\|^2}{d} \to 1 \quad \text{as } d\to\infty
$$

**Implication:** Distances in high-dimensional Gaussian embeddings concentrate around $\sqrt{d}$, reducing the discriminative power of similarity search.

---

**4. Nearest-neighbor distance growth**  
For $n$ i.i.d. points uniformly distributed in $[0,1]^d$, the volume of a ball of radius $r$ is $V_d(r) \approx c_d r^d$. The expected nearest-neighbor distance satisfies:

$$
n \cdot V_d(r) \approx \log n \quad \Rightarrow \quad r \approx \left( \frac{\log n}{n} \right)^{1/d}
$$

As $d$ increases, $r$ approaches 1 very quickly.  

**Implication:** $k$-NN classifiers fail in high dimensions since neighborhoods become empty or cover almost the whole space.

<br>

# Part II. Preprocessing, Normalization, and Scaling

**1. Standardization and condition number**  
Standardization rescales features to have zero mean and unit variance. This improves numerical stability because the condition number of the Gram matrix

$$
\kappa(X^\top X) = \frac{\lambda_{\max}(X^\top X)}{\lambda_{\min}(X^\top X)}
$$

is minimized when columns are scaled equally. Poorly scaled features inflate $\kappa$, leading to unstable least-squares solutions.

---

**2. Centering in PCA**  
Covariance matrix of data $\{x_i\}$ without centering is:

$$
\Sigma = \frac{1}{n} \sum_i x_i x_i^\top
$$

If we center with $\bar{x}$:

$$
\Sigma_c = \frac{1}{n} \sum_i (x_i - \bar{x})(x_i - \bar{x})^\top
$$

Without centering, the first principal component may align with the mean vector instead of true directions of variance. Thus centering is essential.

---

**3. Mahalanobis distance**  
For covariance matrix $\Sigma$:

$$
d_M(x,y) = \sqrt{(x-y)^\top \Sigma^{-1} (x-y)}
$$

If we whiten the data by $z = \Sigma^{-1/2} x$, then:

$$
d_M(x,y) = \| \Sigma^{-1/2}(x-y) \|_2 = \|z_x - z_y\|_2
$$

Hence Mahalanobis distance is just Euclidean distance in the whitened space.

---

**4. Scaling methods**  
- **Min–max scaling:**  

$$
x' = \frac{x - \min(x)}{\max(x) - \min(x)}
$$

Useful when features are bounded and distribution is approximately uniform.  

- **Z-score standardization:**  

$$
x' = \frac{x - \mu}{\sigma}
$$

Appropriate when features are Gaussian-like.  

- **Robust scaling:**  

$$
x' = \frac{x - \text{median}(x)}{\text{IQR}(x)}
$$

Effective for heavy-tailed or outlier-prone distributions.

---

**5. Data leakage**  
Leakage occurs when test data information is used during preprocessing. Example: scaling entire dataset before splitting.  

Correct procedure: fit scaler on training data only, then apply to test data. This ensures no information about the test set influences training.

<br>

# Part III. Inner Product Spaces and Similarity

**1. Distance-inner product relationship**

$$
\|x-y\|^2 = \|x\|^2 + \|y\|^2 - 2\langle x, y \rangle
$$

This identity shows that similarity (inner product) and distance are directly related.

---

**2. Cosine similarity**  
Cosine similarity is:

$$
\cos(x,y) = \frac{\langle x,y \rangle}{\|x\|\|y\|}
$$

Thus, Euclidean distance and cosine similarity both rely on inner products but differ in their normalization.

---

**3. Example**  
Let $x=(1,0)$, $y=(0,1)$, $z=(10,0)$.  

- Euclidean distances:  

$$
\|x-y\|=\sqrt{2}, \quad \|x-z\|=9
$$

So $y$ is closer to $x$ than $z$.  

- Cosine similarities:  

$$
\cos(x,y)=0, \quad \cos(x,z)=1
$$

So $z$ is more similar to $x$.  

**Interpretation:** Euclidean distance cares about magnitude, cosine similarity only about direction.

---

**4. Bregman divergence**  
Given convex function $\phi$:

$$
D_\phi(x,y) = \phi(x) - \phi(y) - \langle \nabla\phi(y), x-y \rangle
$$

It is non-negative but not symmetric and does not satisfy the triangle inequality, so it is not a metric.  

Compared to Mahalanobis distance, Bregman divergences generalize beyond quadratic forms.

---

**5. Gaussian RBF kernel**  

$$
k(x,y) = \exp\left(-\frac{\|x-y\|^2}{2\sigma^2}\right)
$$

It is positive semidefinite because it is the Fourier transform of a Gaussian (Bochner’s theorem).  

**Intuition:** RBF corresponds to embedding points into an infinite-dimensional Hilbert space where similarity decays smoothly with distance.

<br>

# Part IV. Metric Learning and Representation

**1. Mahalanobis metric validity**  
If $M\succeq 0$, then $M=L^\top L$. The Mahalanobis distance is:

$$
d_M(x,y) = \sqrt{(x-y)^\top M (x-y)} = \|L(x-y)\|_2
$$

This satisfies non-negativity, symmetry, and triangle inequality, thus is a valid metric.

---

**2. Applications of metric learning**  
- **Information retrieval:** Learn embeddings where semantically similar documents are close. Improves search ranking.  
- **Face recognition:** Learn embeddings where same-identity faces are close and different identities are far apart (e.g., using triplet loss).

---

**3. Metric vs kernel learning**  
- Metric learning learns a transformation $L$ for distances in input space.  
- Kernel learning defines similarity via  

$$
k(x,y)=\langle \phi(x),\phi(y)\rangle
$$  

They are dual perspectives: metric learning shapes geometry in input space, kernel learning defines geometry in feature space.

---

# Part V. Critical Reflection

**1. High-dimensional small-sample challenges**  
With $d=10,000$, $n=500$:  
- Overfitting: $d \gg n$ means models can memorize.  
- Ill-conditioning: covariance matrix rank $\leq n$, leading to singularity.  
- Curse of dimensionality: distances concentrate.  

**Solutions:**  
- **Dimensionality reduction:** Apply PCA to project to $m \ll n$. Reduces variance and improves stability.  
- **Regularization:** Ridge regression adds $\lambda I$ to $X^\top X$, improving condition number.  

---

**2. Deep learning representation vs preprocessing**  
Advantages:  
- Learns task-specific embeddings automatically.  
- Captures nonlinear structure beyond manual feature scaling.  

Risks:  
- Interpretability: learned features are opaque.  
- Robustness: adversarial vulnerability and sensitivity to data shifts.  